<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.3">
<title>NLP Based Analysis of Literary Texts (M 5.2.3)</title>
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Remove comment around @import statement below when using as a custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
[hidden],template{display:none}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
body{margin:0}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}
input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*:before,*:after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
body{-webkit-font-smoothing:antialiased}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.spread{width:100%}
p.lead,.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{font-size:1.21875em;line-height:1.6}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #ddddd8;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol,ul.no-bullet,ol.no-bullet{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.no-bullet{list-style:none}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite:before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media only screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7;font-weight:bold}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
body{tab-size:4}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix:before,.clearfix:after,.float-group:before,.float-group:after{content:" ";display:table}
.clearfix:after,.float-group:after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menu{color:rgba(0,0,0,.8)}
b.button:before,b.button:after{position:relative;top:-1px;font-weight:400}
b.button:before{content:"[";padding:0 3px 0 2px}
b.button:after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header:before,#header:after,#content:before,#content:after,#footnotes:before,#footnotes:after,#footer:before,#footer:after{content:" ";display:table}
#header:after,#content:after,#footnotes:after,#footer:after{clear:both}
#content{margin-top:1.25em}
#content:before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #ddddd8}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #ddddd8;padding-bottom:8px}
#header .details{border-bottom:1px solid #ddddd8;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span:before{content:"\00a0\2013\00a0"}
#header .details br+span.author:before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark:before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber:after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #ddddd8;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #efefed;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media only screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #efefed;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #efefed;left:auto;right:0}}
@media only screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
.sect1{padding-bottom:.625em}
@media only screen and (min-width:768px){.sect1{padding-bottom:1.25em}}
.sect1+.sect1{border-top:1px solid #efefed}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor:before,h2>a.anchor:before,h3>a.anchor:before,#toctitle>a.anchor:before,.sidebarblock>.content>.title>a.anchor:before,h4>a.anchor:before,h5>a.anchor:before,h6>a.anchor:before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock>caption.title{white-space:nowrap;overflow:visible;max-width:0}
.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>.paragraph:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #ddddd8;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;padding:1em;font-size:.8125em}
.literalblock pre.nowrap,.literalblock pre[class].nowrap,.listingblock pre.nowrap,.listingblock pre[class].nowrap{overflow-x:auto;white-space:pre;word-wrap:normal}
@media only screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}
@media only screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}
.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]:before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]:before{display:block}
.listingblock.terminal pre .command:before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt]):before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0;line-height:1.45}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #ddddd8}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock blockquote p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote:before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.5em;margin-right:.5ex;text-align:right}
.quoteblock .quoteblock{margin-left:0;margin-right:0;padding:.5em 0;border-left:3px solid rgba(0,0,0,.6)}
.quoteblock .quoteblock blockquote{padding:0 0 0 .75em}
.quoteblock .quoteblock blockquote:before{display:none}
.verseblock{margin:0 1em 1.25em 1em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract{margin:0 0 1.25em 0;display:block}
.quoteblock.abstract blockquote,.quoteblock.abstract blockquote p{text-align:left;word-spacing:0}
.quoteblock.abstract blockquote:before,.quoteblock.abstract blockquote p:first-of-type:before{display:none}
table.tableblock{max-width:100%;border-collapse:separate}
table.tableblock td>.paragraph:last-child p>p:last-child,table.tableblock th>p:last-child,table.tableblock td>p:last-child{margin-bottom:0}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all th.tableblock,table.grid-all td.tableblock{border-width:0 1px 1px 0}
table.grid-all tfoot>tr>th.tableblock,table.grid-all tfoot>tr>td.tableblock{border-width:1px 1px 0 0}
table.grid-cols th.tableblock,table.grid-cols td.tableblock{border-width:0 1px 0 0}
table.grid-all *>tr>.tableblock:last-child,table.grid-cols *>tr>.tableblock:last-child{border-right-width:0}
table.grid-rows th.tableblock,table.grid-rows td.tableblock{border-width:0 0 1px 0}
table.grid-all tbody>tr:last-child>th.tableblock,table.grid-all tbody>tr:last-child>td.tableblock,table.grid-all thead:last-child>tr>th.tableblock,table.grid-rows tbody>tr:last-child>th.tableblock,table.grid-rows tbody>tr:last-child>td.tableblock,table.grid-rows thead:last-child>tr>th.tableblock{border-bottom-width:0}
table.grid-rows tfoot>tr>th.tableblock,table.grid-rows tfoot>tr>td.tableblock{border-width:1px 0 0 0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot{border-width:1px 0}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.unstyled,ol.unnumbered,ul.checklist,ul.none{list-style-type:none}
ul.unstyled,ol.unnumbered,ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1em;font-size:.85em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{width:1em;position:relative;top:1px}
ul.inline{margin:0 auto .625em auto;margin-left:-1.375em;margin-right:0;padding:0;list-style:none;overflow:hidden}
ul.inline>li{list-style:none;float:left;margin-left:1.375em;display:block}
ul.inline>li>*{display:block}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist>table tr>td:first-of-type{padding:0 .75em;line-height:1}
.colist>table tr>td:last-of-type{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left,.imageblock[style*="float: left"]{margin:.25em .625em 1.25em 0}
.imageblock.right,.imageblock[style*="float: right"]{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em 0;border-width:1px 0 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;text-indent:-1.05em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note:before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip:before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning:before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution:before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important:before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@media print{@page{margin:1.25cm .75cm}
*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare):after,a[href^="https:"]:not(.bare):after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]:after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #ddddd8!important;padding-bottom:0!important}
.sect1{padding-bottom:0!important}
.sect1+.sect1{border:0!important}
#header>h1:first-child{margin-top:1.25rem}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em 0}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span:before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]:before{display:block}
#footer{background:none!important;padding:0 .9375em}
#footer-text{color:rgba(0,0,0,.6)!important;font-size:.9em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
</style>
<style>
/* Stylesheet for CodeRay to match GitHub theme | MIT License | http://foundation.zurb.com */
/*pre.CodeRay {background-color:#f7f7f8;}*/
.CodeRay .line-numbers{border-right:1px solid #d8d8d8;padding:0 0.5em 0 .25em}
.CodeRay span.line-numbers{display:inline-block;margin-right:.5em;color:rgba(0,0,0,.3)}
.CodeRay .line-numbers strong{color:rgba(0,0,0,.4)}
table.CodeRay{border-collapse:separate;border-spacing:0;margin-bottom:0;border:0;background:none}
table.CodeRay td{vertical-align: top;line-height:1.45}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.line-numbers>pre{padding:0;color:rgba(0,0,0,.3)}
table.CodeRay td.code{padding:0 0 0 .5em}
table.CodeRay td.code>pre{padding:0}
.CodeRay .debug{color:#fff !important;background:#000080 !important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:#000080}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:#008080}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:#008080}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#000}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:#008080}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword {color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:#008080}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>NLP Based Analysis of Literary Texts (M 5.2.3)</h1>
<div id="toc" class="toc2">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#_setting_up_the_pipeline">1. Setting Up the Pipeline</a>
<ul class="sectlevel2">
<li><a href="#_system_requirements">1.1. System Requirements</a></li>
<li><a href="#_java_installation">1.2. Java Installation</a></li>
<li><a href="#_pipeline_download">1.3. Pipeline Download</a></li>
</ul>
</li>
<li><a href="#RunningthePipeline">2. Running the Pipeline</a>
<ul class="sectlevel2">
<li><a href="#_using_the_command_line">2.1. Using the Command Line</a></li>
<li><a href="#_processing_a_textfile">2.2. Processing a Textfile</a></li>
<li><a href="#_file_reader">2.3. File Reader</a></li>
<li><a href="#_language">2.4. Language</a></li>
<li><a href="#_command_line_options">2.5. Command Line Options</a></li>
<li><a href="#_troubleshooting">2.6. Troubleshooting</a></li>
</ul>
</li>
<li><a href="#_available_components">3. Available Components</a>
<ul class="sectlevel2">
<li><a href="#_segmentation">3.1. Segmentation</a></li>
<li><a href="#_part_of_speech_tagging">3.2. Part-of-Speech Tagging</a></li>
<li><a href="#_lemmatization">3.3. Lemmatization</a></li>
<li><a href="#_constituency_and_dependency_parsing">3.4. Constituency and Dependency Parsing</a></li>
<li><a href="#_named_entity_recognition">3.5. Named Entity Recognition</a></li>
<li><a href="#_semantic_role_labeling">3.6. Semantic Role Labeling</a></li>
</ul>
</li>
<li><a href="#_configuring_the_pipeline">4. Configuring the Pipeline</a>
<ul class="sectlevel2">
<li><a href="#_run_the_full_pipeline">4.1. Run the Full Pipeline</a></li>
<li><a href="#_write_your_own_config_file">4.2. Write Your Own Config File</a></li>
<li><a href="#_building_your_own">4.3. Building Your Own</a></li>
<li><a href="#_using_treetagger">4.4. Using TreeTagger</a></li>
</ul>
</li>
<li><a href="#OutputFormat">5. Output Format</a>
<ul class="sectlevel2">
<li><a href="#_specification">5.1. Specification</a></li>
<li><a href="#ReadingtheOutput">5.2. Reading the Output</a></li>
<li><a href="#_further_examples">5.3. Further Examples</a></li>
<li><a href="#_api">5.4. API</a></li>
</ul>
</li>
<li><a href="#_example_recipe_calculate_readability_measures_in_r">6. Example Recipe: Calculate Readability Measures in R</a></li>
<li><a href="#_example_recipe_stylometric_analysis_with_the_stylo_package_in_r">7. Example Recipe: Stylometric Analysis with the "Stylo" Package in R</a>
<ul class="sectlevel2">
<li><a href="#_example_corpus">7.1. Example Corpus</a></li>
<li><a href="#_preparing_descriptive_vocabulary_and_part_of_speech_tags">7.2. Preparing Descriptive Vocabulary and Part-of-Speech Tags</a></li>
<li><a href="#_using_stylo">7.3. Using Stylo</a></li>
</ul>
</li>
<li><a href="#TopicModelinginPython">8. Example Recipe: Topic Modeling in Python</a>
<ul class="sectlevel2">
<li><a href="#_example_corpus_2">8.1. Example Corpus</a></li>
<li><a href="#_setting_up_the_environment">8.2. Setting Up the Environment</a></li>
<li><a href="#_configuration_section">8.3. Configuration Section</a></li>
<li><a href="#_preparing_the_data">8.4. Preparing the Data</a></li>
<li><a href="#_fitting_the_model">8.5. Fitting the Model</a></li>
<li><a href="#_visualization_options">8.6. Visualization Options</a></li>
</ul>
</li>
<li><a href="#_example_recipe_stylometric_classification_in_python">9. Example Recipe: Stylometric Classification in Python</a>
<ul class="sectlevel2">
<li><a href="#_example_corpus_3">9.1. Example Corpus</a></li>
<li><a href="#_setting_up_the_environment_2">9.2. Setting Up the Environment</a></li>
<li><a href="#_feature_selection">9.3. Feature Selection</a></li>
<li><a href="#_preparing_the_data_2">9.4. Preparing the Data</a></li>
<li><a href="#_training_and_evaluating_the_classifier">9.5. Training and Evaluating the Classifier</a></li>
<li><a href="#_prediction_using_the_classifier">9.6. Prediction Using the Classifier</a></li>
</ul>
</li>
<li><a href="#_example_recipe_network_visualization_in_python">10. Example Recipe: Network Visualization in Python</a>
<ul class="sectlevel2">
<li><a href="#_setting_up_the_environment_3">10.1. Setting Up the Environment</a></li>
<li><a href="#_crawling_wikipedia">10.2. Crawling Wikipedia</a></li>
<li><a href="#_using_dkpro_wrapper_and_networkx_to_visualize_networks">10.3. Using DKPro Wrapper and NetworkX to Visualize Networks</a></li>
</ul>
</li>
<li><a href="#_about_this_tutorial">11. About this Tutorial</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>The following tutorial describes how to analyze texts, by first
<strong><a href="#RunningthePipeline">generating linguistic annotations</a></strong> with a simple, single java program that bundles
the abilities of several state-of-the-art NLP (Natural Language
Processing) tools, and then accessing the annotations provided in a
<strong><a href="#OutputFormat">standardized output format</a></strong> for complex
<strong><a href="#TopicModelinginPython">empirical analyses of text style and content</a></strong> in a scripting language. The
workflows are based on a tool pipeline packaged from
the <strong><a href="http://dkpro.org">DKPro</a></strong> framework for
natural language processing into a single file, thus providing
convenient access to many important DKPro features even for users with
virtually no work experience with the framework. The example recipes
further below describe how to use the pipeline&#8217;s output for advanced
analytic procedures.</p>
</div>
<!-- toc disabled -->
</div>
</div>
<div class="sect1">
<h2 id="_setting_up_the_pipeline">1. Setting Up the Pipeline</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_system_requirements">1.1. System Requirements</h3>
<div class="paragraph">
<p>To run the pipeline properly, a system equipped with and able to handle
at least 4 GB RAM is recommended. The following operating systems have
been tested:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>macOS (10.10 - 10.13)</p>
</li>
<li>
<p>Linux (Ubuntu 14.04 - 17.10)</p>
</li>
<li>
<p>Windows 7 - 10</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Furthermore, the pipeline depends on an <strong>internet connection</strong> when
running to download the models for the current configuration. <strong>It does
not work offline!</strong></p>
</div>
</div>
<div class="sect2">
<h3 id="_java_installation">1.2. Java Installation</h3>
<div class="paragraph">
<p>The following step installs the base system requirements needed to run
DKPro Core pipelines on your machine. This needs to be performed only
once. Download and install the latest Java SE Runtime Environment (at least Java 1.8) from
the <a href="http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html">Oracle
Java Site</a>, then follow the
<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html">installation
instructions</a> for your operating system. You can check your current Java version by running <code>java -version</code> in your command line.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pipeline_download">1.3. Pipeline Download</h3>
<div class="paragraph">
<p>When the Java environment is prepared, you
can <a href="https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper">download the latest binary</a>. Select
the file named ddw-0.4.7.zip and unpack it somewhere
easily accessible. As a next step we need to navigate to this folder
using the command line.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="RunningthePipeline">2. Running the Pipeline</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_using_the_command_line">2.1. Using the Command Line</h3>
<div class="paragraph">
<p>The DKPro pipeline does not have a graphical user
interface <a href="http://en.wikipedia.org/wiki/Graphical_user_interface">(GUI)</a>.
Therefore you have to use the pipeline (both setting up and processing
data) with the command prompt. In all versions of the <strong>Windows</strong>
operating system, pressing the Windows key + "R" should launch the
command prompt. Otherwise, the command prompt can be launched</p>
</div>
<div class="ulist">
<ul>
<li>
<p>in Windows 7 by clicking on the "Start"-button, type "command" in the
search box and click on "Command Prompt"</p>
</li>
<li>
<p>in Windows 8 with a right-click on the “Start”-button, choosing “run",
and typing “cmd” in the search box. Alternatively type "cmd" in the
"Search".</p>
</li>
<li>
<p>in Windows 10 by typing "cmd.exe" into the search box on the taskbar and selecting the first option.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Navigate to the directory that contains the DKPro-pipeline. For example,
if you are using windows and keeping your pipeline in folder named
"DKPro" on drive "D:", by typing,</p>
</div>
<div class="listingblock">
<div class="content">
<pre>cd D:\DKPro</pre>
</div>
</div>
<div class="paragraph">
<p>and press enter.</p>
</div>
<div class="paragraph">
<p>Alternatively the directory can be accessed instantly by holding Shift + right-clicking the folder
needed and selecting "Open command window here".</p>
</div>
</div>
<div class="sect2">
<h3 id="_processing_a_textfile">2.2. Processing a Textfile</h3>
<div class="paragraph">
<p>Now you can process a text file. How to test when you don&#8217;t have any
data? We&#8217;ve prepared a <a href="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/EffiBriestKurz.txt">demonstration text</a> that
can be downloaded and processed via the pipeline. You can compare your
output with <a href="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/EffiBriestKurz.txt.csv">this file</a>.
If you receive an identical output DKPro pipeline works fine on your
computer. There are also a plenty of free texts available
from <a href="http://textgridrep.org/">TextGrid Repository</a> or <a href="http://www.deutschestextarchiv.de/">Deutsches
Textarchiv</a>. <strong>If you do not specify the <code>-language</code> parameter, the pipeline is prepared to analyze English input.</strong> For more details see <a href="#Language">further below</a>.</p>
</div>
<div class="paragraph">
<p>To process data type the following command in the command prompt</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>and press Enter.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -language de -input C:\EffiBriestKurz.txt -output D:\DKPro\Workspace</pre>
</div>
</div>
<div class="paragraph">
<p>If your input and/or output file are located in the current directory you
can type "." instead of the full input- and/or output-path. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -language de -input .\EffiBriestKurz.txt -output .</pre>
</div>
</div>
<div class="paragraph">
<p>The pipeline will process your data and save the output as
a <strong>.csv-File</strong> in the specified folder.  If </p>
</div>
<div class="listingblock">
<div class="content">
<pre>File written, DONE </pre>
</div>
</div>
<div class="paragraph">
<p>is shown on your command prompt everything has worked well. To see final
results check the output-file in your specified output folder.<br>
<br>
<strong>Important Note:</strong> Depending on the configuration of your system and
the size of the input file processing <strong>may take some time</strong>, e.g. even
a test file of 630 words may easily take 1-2 minutes, even if 4 GB RAM
are allocated to the task.</p>
</div>
</div>
<div class="sect2">
<h3 id="_file_reader">2.3. File Reader</h3>
<div class="paragraph">
<p>You can process either single files or also all files inside a directory. Patterns can be used to select specific files that should be processed.</p>
</div>
<div class="sect3">
<h4 id="_text_reader_xml_reader">2.3.1. Text Reader &amp; XML Reader</h4>
<div class="paragraph">
<p>The DARIAH-DKPro-Wrapper implements two base readers, one text reader and one XML-file reader. You can specify the reader that should be used with the <code>-reader</code> parameter. By default, the text reader is used. To use the XML reader, run the pipeline in the following way:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -reader xml -input file.xml -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>The XML reader skips XML tags and processes only text which is inside the XML tags. The XPath to each tag is conserved and stored in the column <strong>SectionId</strong> in the output format.</p>
</div>
</div>
<div class="sect3">
<h4 id="_reading_directories">2.3.2. Reading Directories</h4>
<div class="paragraph">
<p>In case you want to process a collection of texts rather than just a single file, you can do that by providing a path to the <code>-input</code> option. If you run the pipeline in the following way:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -input folder/With/Files/ -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>the pipeline will process all files with a <em>.txt</em> extension for the Text-reader. For the XML-reader, it will process all files with a <em>.xml</em> extension.</p>
</div>
<div class="paragraph">
<p>You can speficy also patterns to read in only certain files or files with certain extension. For example to read in only <em>.tei</em> with the XML reader, you must start the pipeline in the following way:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -reader xml -input "folder/With/Files/*.tei" -output folder</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Note:</strong> If you use patterns (i.e. paths containing an *), you must set it into quotation marks to prevent shell globbing.</p>
</div>
<div class="paragraph">
<p>To read all files in all subfolders, you can use a pattern like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -input "folder/With/Subfolders/\**/*.txt" -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>This will read in all <em>.txt</em> files in all subfolders. Note that the subfolder path will not be maintained in the output folder.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_language">2.4. Language</h3>
<div class="paragraph">
<p>You can change the language by specifying the language parameter for the pipeline. Support for the following languages are included in the current version of the DARIAH-DKPro-Wrapper: German (de), English (en), Spanish (es), and French (fr). If you want to work with Bulgarian (bg), Danish (da), Estonian (et), Finnish (fi), Galician (gl), Latin (la), Mongolian (mn), Polish (pl), Russian (ru), Slovakian (sk) or Swahili (sw) input, you have to install <a href="#UsingTreeTagger">TreeTagger</a> first. To run the pipeline for German, execute the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -language de -input file.txt -output folder</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_command_line_options">2.5. Command Line Options</h3>
<div class="sect3">
<h4 id="_help">2.5.1. Help</h4>
<div class="paragraph">
<p>The pipeline provides a help function that can be accessed on the
command line with the "-help" option. Run java -jar  ddw-{version}.jar -help to get an overview of the possible command line arguments:</p>
</div>
<div class="listingblock">
<div class="content">
<pre> -config &lt;path&gt;     Config file
 -help              print this message
 -input &lt;path&gt;      Input path
 -language &lt;lang&gt;   Language code for input file (default: en)
 -output &lt;path&gt;     Output path
 -reader &lt;reader&gt;   Either text (default) or xml
 -resume            Already processed files will be skipped</pre>
</div>
</div>
<div class="paragraph">
<p>The pipeline supports a resume function. By adding the <code>-resume</code> argument to the execution of the pipeline, all files that were previously processed and have an according <code>.csv</code>-file in the output folder will be skipped.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_troubleshooting">2.6. Troubleshooting</h3>
<div class="paragraph">
<p>If there is no output in your output folder and your command prompt
shows</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Exception in thread "main" java.lang.OutOfMemoryError: Java heap space or The specified size exceeds the maximum representable size. Error: Could not create the Java Virtual Machine</pre>
</div>
</div>
<div class="paragraph">
<p>you need to <strong>check the size of virtual memory</strong>. Depending on the
maximum size of your RAM you should allocate 4GB or 6GB. The
flag <strong>Xms</strong> specifies the initial memory allocation pool for a Java
Virtual Machine (JVM). After adapting Windows' virtual memory type the
following in the command prompt:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java –Xms -jar ddw-0.4.7.jar -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>and press enter.</p>
</div>
<div class="paragraph">
<p>For example, if you allocated 4GB then type:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xms4g -jar ddw-0.4.7.jar -input EffiBriestKurz.txt -output D:\DKPro\Workspace</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Note:</strong> Allocating too much virtual memory can slow down your system -
4GB or 6GB should be enough for most processing operations.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_available_components">3. Available Components</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As mentioned above, the pipeline contains a number of components</p>
</div>
<div class="sect2">
<h3 id="_segmentation">3.1. Segmentation</h3>
<div class="paragraph">
<p>Segmentation is the task of dividing running text into units like
sentences and words.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Word segmentation, also called tokenization, is the process of finding
word boundaries - in its simplest form, by using the blanks in-between
words as delimiters. However, there are languages that do not support
this, such as Chinese or Japanese.</p>
</li>
<li>
<p>Sentence segmentation is the process of splitting text based on
sentence limiting punctuation e.g. periods, question marks etc. Note
that the periods are sometimes not the markers of sentence boundaries
but the markers of abbreviations.</p>
</li>
<li>
<p>Besides, there are many other different segmentations on the basis of
different purposes such as discourse segmentation (separating a document
into a linear sequence of subtopics), Paragraph segmentation (which
automatically break the text up into paragraphs) and so forth.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_part_of_speech_tagging">3.2. Part-of-Speech Tagging</h3>
<div class="paragraph">
<p>Labeling every word and punctuation mark (token) in a text corpus with a
predefined set of part-of-speech tags (standardized abbreviations) or
other syntactic class markers, is called Part of Speech Tagging. Usually
the output of a POS-Tagger will look like this (showing also DKPro&#8217;s
CPOS column - a universal coarse grained tag set designed for the
interoperability of components in different languages):</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 33%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Token</th>
<th class="tableblock halign-left valign-top">CPOS</th>
<th class="tableblock halign-left valign-top">POS</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Auf</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">PP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">APPR</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">einmal</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ADV</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ADV</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">schien</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">V</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VVFIN</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">die</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ART</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ART</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sonne</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">NN</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">NN</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">durchzudringen</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">V</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VVIZU</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Most tagging algorithms fall into one of two classes: rule-based taggers
and probabilistic or stochastic taggers. Rule-based taggers
generally involve a large database of hand-written disambiguation rules.
Stochastic taggers generally resolve tagging ambiguities by using a
training corpus to compute the probability of a given word having a
given tag in a given context. Additionally there is an approach to
tagging called the transformation-based tagger, or the Brill tagger,
which shares features of both above tagging architectures.</p>
</div>
</div>
<div class="sect2">
<h3 id="_lemmatization">3.3. Lemmatization</h3>
<div class="paragraph">
<p>Mapping all different inflected word forms to one lemma is called
lemmatization. It is related to stemming, an approach that tries to
recognize derivational parts of a word to cut them off, leaving the stem
as a result. In both cases, an amount of words are grouped together in a
specific way. In stemming, the words are reduced to its stem. In
lemmatization they are reduced to their common base lemma. The
difference is, that a found stem would include every word containing the
stem, but no other related words, as is the case with irregular verbs.
Furthermore, the stem does not have to be a legit word, as long as it
constitutes the common base morpheme. On the other hand, a lemma will
most likely be the infinitive form of a verb or unmodified version of
the word in question. Looking back to the example of stemming: Stemming
of the words <em>gone</em>, <em>going</em>, and <em>goes</em> will not include the
related term <em>went</em>, which would be the case after lemmatization.</p>
</div>
</div>
<div class="sect2">
<h3 id="_constituency_and_dependency_parsing">3.4. Constituency and Dependency Parsing</h3>
<div class="paragraph">
<p>Parsing is the main task behind breaking down a text into its more basic
pieces and structures. A parser will take some input text and find
specific structures, according to the preset rules, or syntax. Every
conversion from one text-structure to another relies on parsing. If an
algorithm takes a text and produces an output that contains the words
with their corresponding part of speech (POS) tags, we can say, that the
algorithm parsed the text finding words and adding POS information.
Parsing is therefore the root of many kinds of linguistic analyses
producing many sorts of structured output. </p>
</div>
<div class="paragraph">
<p>The idea of constituency is that groups of words may behave as a single
unit or phrase, called a constituent such as <em>the house</em>, or <em>a
well-weathered three-story structure</em>. The task of constituency parsing
is to automatically find those words, which form the constituents. The
final tree structure consists of final and non-final nodes. The final
nodes are the words of the text that was parsed. The non-final nodes
define the type of the phrase represented below the node.</p>
</div>
<div class="paragraph">
<p>In contrast, the notion of dependency foregrounds the words themselves
and displays them as connected to each other by direct links. The
structural center of the sentence is the verb to which every other word
is (in)directly connected. Compared with the constituency form of
representation, a dependency tree can be described as flat. The lack of
phrase structure makes dependency grammars a good match for languages
with free word order, such as Czech and Turkish.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/blob/master/doc/content/constituency_dependency.jpg" alt="Parsing"></span></p>
</div>
<div class="paragraph">
<p><a href="https://commons.wikimedia.org/wiki/File:Wearetryingtounderstandthedifference_(2).jpg">Dependency
vs. constituency</a> by
<a href="https://commons.wikimedia.org/w/index.php?title=User:Tjo3ya&amp;action=edit&amp;redlink=1">Tjo3ya</a>
(<a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>)</p>
</div>
</div>
<div class="sect2">
<h3 id="_named_entity_recognition">3.5. Named Entity Recognition</h3>
<div class="paragraph">
<p>Named entity recognition (NER) is a pre-processing step in most
information extraction tasks. Named entity stands for the text block,
which refers a name. NER describes the task of finding all names in one
text and categorizing them based on their different types, such as
persons, organizations or locations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_semantic_role_labeling">3.6. Semantic Role Labeling</h3>
<div class="paragraph">
<p>Semantic role labeling (SRL, also: thematic role labeling, case role
assignment) refers to a parsing approach that aims towards detecting all
arguments of a verb. Ideally, it is able to assign appropriate semantic
roles to its arguments (such as <em>agent, patient, </em>or <em>instrument</em>),
thus preparing for a semantic interpretation of the sentence.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configuring_the_pipeline">4. Configuring the Pipeline</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_run_the_full_pipeline">4.1. Run the Full Pipeline</h3>
<div class="paragraph">
<p>By default, the pipeline runs in a light mode, the memory and time intensive components for parsing and semantic role labeling are <strong>disabled</strong>.</p>
</div>
<div class="paragraph">
<p>If you like to use them, feel free to enable them in the <code>default.properties</code> or create a new <code>.properties</code>-File and pass the path to this file via the <code>config</code>-parameter.</p>
</div>
</div>
<div class="sect2">
<h3 id="_write_your_own_config_file">4.2. Write Your Own Config File</h3>
<div class="paragraph">
<p>The pipeline can be configurated via properties-files that are stored in the <code>configs</code> folder. In this folder you find a <code>default.properties</code>, the most basic configuration file. For the different supported languages, you can find further properties-files, for example <code>default_de.properties</code> for German, <code>default_en.properties</code> for English and so on.</p>
</div>
<div class="paragraph">
<p>If you like to write your own config file, just create your own <code>.properties</code> file. You have a range of possibilities to modify the pipeline for your purpose as you can see <a href="https://dkpro.github.io/dkpro-core/releases/1.7.0/apidocs/index.html">here</a>.</p>
</div>
<div class="paragraph">
<p>For clarification have a look at line 3 to 13 in <code>default.properties</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>###################################
# Segmentation
###################################
useSegmenter = true # line 6
segmenter = de.tudarmstadt.ukp.dkpro.core.opennlp.OpenNlpSegmenter # line 7

# Possible values for segmenter:
# - de.tudarmstadt.ukp.dkpro.core.tokit.BreakIteratorSegmenter
# - de.tudarmstadt.ukp.dkpro.core.clearnlp.ClearNlpSegmenter
# - de.tudarmstadt.ukp.dkpro.core.opennlp.OpenNlpSegmenter (default)
# - de.tudarmstadt.ukp.dkpro.core.stanfordnlp.StanfordSegmenter</pre>
</div>
</div>
<div class="paragraph">
<p>The component <a href="#Segmentation">Segmentation</a> is set to boolean true by default (line 6). If you want to disable Segmentation set <code>useSegmenter</code> to <code>false</code>. To use another toolkit than OpenNlpSegmenter (line 7), change the value of <code>segmenter</code> e.g. to <code>de.tudarmstadt.ukp.dkpro.core.stanfordnlp.StanfordSegmenter</code> for the StanfordSegmenter. A more specific modification with argument parameters is explained <a href="#UnderstandingtheArgumentParameter">further below</a>.</p>
</div>
<div class="paragraph">
<p>You can run the pipeline with your <code>.properties</code>-file by setting the command argument.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -config /path/to/my/config/myconfigfile.properties -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>In case you store your <code>myconfigfile.properties</code> in the <code>configs</code> folder, you can run the pipeline via:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -config myconfigfile.properties -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>You can split your config file into different parts and pass them all to the pipeline by seperating the paths using comma or semicolons. The pipeline examines all passed config files and derives the final configuration from all files. The config-file passed as last arguments has the highest priority, i.e. it can overwrite the values for all previous config files:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -config myfile1.properties,myconfig2.properties,myfile3.properties -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Note:</strong> The system always uses the default.properties and default_[langcode].properties as basic configuration files. All further config files are added on top of these files.</p>
</div>
<div class="paragraph">
<p>In case you like to use the <em>full</em>-version and also want to change the POS-tagger, you can run the pipeline in the following way:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -config myFullVersion.properties,myPOSTagger.properties -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>In <code>myPOSTagger.properties</code> you just add the configuration for the different POS-tagger.</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> The properties-files must use the <a href="https://en.wikipedia.org/wiki/ISO/IEC_8859-1">ISO-8859-1</a> encoding. If you like to include <a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8</a> characters, you must encode them using \u[HEXCode].</p>
</div>
<div class="sect3">
<h4 id="_understanding_the_argument_parameter">4.2.1. Understanding the Argument Parameter</h4>
<div class="paragraph">
<p>A parameter is a special variable, consisting one or more arguments, provided to the subroutine. Most components of the DKPro pipeline can be equipped with arguments to specify for example the model that should be used. A list of possible arguments is available <a href="https://dkpro.github.io/dkpro-core/releases/1.7.0/apidocs/constant-values.html">here</a> in the column <strong>Constant Field</strong> or rather <strong>Value</strong>. Arguments are passed to the pipeline in a 3 tuple format:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The first tuple corresponds to the value of the Constant Field, e.g. writeDependency.</p>
</li>
<li>
<p>The second tuple declares the data type of the following tuple, e.g. boolean. As type you can use <em>boolean</em>, <em>integer</em>, and <em>string</em>.</p>
</li>
<li>
<p>The third tuple has to be a concrete data type value, e.g. false.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the <code>default.properties</code> you can find the following line:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>constituencyParserArguments = writeDependency,boolean,false</pre>
</div>
</div>
<div class="paragraph">
<p>Here we specify the argument <strong>writeDependency</strong> with the boolean value <strong>false</strong>. This suggests, that no dependency annotations will be created.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_building_your_own">4.3. Building Your Own</h3>
<div class="paragraph">
<p>For creating your own pipeline the latest version of Java SDK (1.8 or
higher), Eclipse (4.3.x), the Maven Integration for Eclipse (M2E) plugin
and the DKPro Core ASL 1.8.0 or higher have to be installed on your
computer. For further information
see <a href="https://dkpro.github.io/dkpro-core/pages/java-intro/">First
Programming Steps with DKPro Core</a>.</p>
</div>
<div class="paragraph">
<p>Some of the analysis components can be run with different models. For
processing you can choose the component and the model that suits your
interests the most
from <a href="https://dkpro.github.io/dkpro-core/releases/1.7.0/components/">this
list</a>. For example, if you want to classify entities such as the names
of persons, locations, expressions of times, organizations and so on
there are two selectable components. StanfordNamedEntityRecognizer and
OpenNlpNameFinder both are suitable for Named Entity Recognition. But if
you are working with a German text StanfordNamedEntityRecognizer would
be the better choice as you see in
the <a href="https://dkpro.github.io/dkpro-core/releases/1.7.0/models/">list
of models</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_using_treetagger">4.4. Using TreeTagger</h3>
<div class="paragraph">
<p>Due to copyright issues, TreeTagger cannot directly be accessed from the DKPro repository. Instead, you have first to download and to install TreeTagger to able to use it with DKPro.</p>
</div>
<div class="sect3">
<h4 id="_installation">4.4.1. Installation</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Go to the <a href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/">TreeTagger website</a></p>
</li>
<li>
<p>From the download section, download the correct tagger package, i.e. PC-Linux, OS X or Windows</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Extract the .tar.gz and .zip archive, respectively</p>
</li>
<li>
<p>Create a new directory <code>tree-tagger</code> containing two folders <code>bin</code> and <code>lib</code> on your hard drive, e.g. <code>C:/tree-tagger/bin</code> and <code>C:/tree-tagger/lib</code></p>
</li>
<li>
<p>Copy the <code>tree-tagger/bin/tree-tagger</code> file <strong>from the previously downloaded archive</strong> to your recently created directory <code>tree-tagger</code> into the folder <code>bin</code></p>
</li>
</ol>
</div>
</li>
<li>
<p>From the parameter file section, download the correct model. For the example below download Latin parameter file (latin-par-linux-3.2-utf8.bin.gz)</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Unzip the file (e.g. <code>gunzip latin-par-linux-3.2-utf8.bin.gz</code> or alternatively use a program like 7zip or WinRar)</p>
</li>
<li>
<p>Copy the extracted file latin.par into the folder <code>lib</code> in your created directory <code>tree-tagger</code></p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_configuration">4.4.2. Configuration</h4>
<div class="paragraph">
<p>After downloading the correct executable and correct model, we must configure our pipeline in order to be able to use TreeTagger. You can find an example configuration in the <em>configs</em> folder <em>treetagger-example.properties</em>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>posTagger =  de.tudarmstadt.ukp.dkpro.core.treetagger.TreeTaggerPosTagger
posTaggerArguments = executablePath,string,C:/tree-tagger/bin/tree-tagger.exe,\
	modelLocation,string,C:/tree-tagger/lib/latin.par,\
	modelEncoding,string,utf-8

# Treetagger adds lemmas, no need for an additional lemmatizer
useLemmatizer = false</pre>
</div>
</div>
<div class="paragraph">
<p>Change the paths for the parameter <em>executablePath</em> and <em>modelLocation</em> to the correct paths on your machine. You can then use TreeTagger in your pipeline using the <code>-config</code> argument:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Xmx4g -jar ddw-0.4.7.jar -config treetagger-example.properties -language la -input file.txt -output folder</pre>
</div>
</div>
<div class="paragraph">
<p>Check the output of the pipeline that TreeTagger is used. The output of your pipeline should look something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>POS-Tagger: true
POS-Tagger: class de.tudarmstadt.ukp.dkpro.core.treetagger.TreeTaggerPosTagger
POS-Tagger: executablePath, C:/tree-tagger/bin/tree-tagger.exe, modelLocation, C:/tree-tagger/lib/latin.par, modelEncoding, utf-8</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="OutputFormat">5. Output Format</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_specification">5.1. Specification</h3>
<div class="paragraph">
<p>The wrapper&#8217;s output format is described in <a href="http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-20.pdf">Fotis Jannidis, Stefan Pernes, Steffen Pielström, Isabella Reger, Nils Reimers, Thorsten Vitt: "DARIAH-DKPro-Wrapper Output Format (DOF) Specification". DARIAH-DE Working Papers Nr. 20. Göttingen: DARIAH-DE, 2016. URN: urn:nbn:de:gbv:7-dariah-2016-6-2</a>.</p>
</div>
<div class="paragraph">
<p>Example
(from <a href="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/EffiBriestKurz.txt.csv">EffiBriestKurz.txt.csv</a>):</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/effibriest_screenshot.png" alt="EffiBriestKurz.txt.csv"></span></p>
</div>
</div>
<div class="sect2">
<h3 id="ReadingtheOutput">5.2. Reading the Output</h3>
<div class="sect3">
<h4 id="_r">5.2.1. R</h4>
<div class="paragraph">
<p>In R, a simple reader can be written as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">df = read.table(&quot;./data/EffiBriestKurz.txt.csv&quot;,    # or whatever file you want to read
                header = T,                         # first line as headers
                fill = T)                           # fill empty cells to avoid errors</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_python">5.2.2. Python</h4>
<div class="paragraph">
<p>In Python, you can use the following code to ingest the output file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">pandas</span> <span class="keyword">as</span> pd

<span class="keyword">import</span> <span class="include">csv</span>

df = pd.read_csv(<span class="string"><span class="delimiter">&quot;</span><span class="content">EffiBriestKurz.txt.csv</span><span class="delimiter">&quot;</span></span>, sep=<span class="string"><span class="delimiter">&quot;</span><span class="char">\t</span><span class="delimiter">&quot;</span></span>, quoting=csv.QUOTE_NONE)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_further_examples">5.3. Further Examples</h3>
<div class="paragraph">
<p>You can also specify a subset of columns to use. Columns are addressed
using their column names.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">columns_input = [<span class="string"><span class="delimiter">'</span><span class="content">SentenceId</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">TokenId</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">Token</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>]

df = df[columns_input]                                     <span class="comment"># use only the selected columns</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Use the pandas.DataFrame.groupby() method to easily access file
contents. The following example shows how to retrieve a sentence.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">sentences = df.groupby(<span class="string"><span class="delimiter">'</span><span class="content">SentenceId</span><span class="delimiter">'</span></span>)                        <span class="comment"># sort by sentence id</span>

sent = sentences.get_group(<span class="integer">10</span>)                              <span class="comment"># get sentence no. 10, returns a smaller dataframe</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Using the same method, you can filter the entire file for a specific
part-of-speech.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">tags = df.groupby(<span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>)                                   <span class="comment"># sort by CPOS values</span>

adj = tags.get_group(<span class="string"><span class="delimiter">'</span><span class="content">ADJ</span><span class="delimiter">'</span></span>)                                 <span class="comment"># get all adjectives</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Filtering for a specific value can also be done within a sentence.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">nn = sent[sent[<span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>] == <span class="string"><span class="delimiter">'</span><span class="content">NN</span><span class="delimiter">'</span></span>]                             <span class="comment"># get nouns from the sentence</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>You can use <a href="http://pandas-docs.github.io/pandas-docs-travis/groupby.html">GroupBy</a>-objects
to process the entire file, e.g. in portions of sentences.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">for</span> sent_id, sent <span class="keyword">in</span> sentences:                             <span class="comment"># iterate through sentences</span>

    <span class="keyword">for</span> tok_id, tok, pos <span class="keyword">in</span> <span class="predefined">zip</span>(sent[<span class="string"><span class="delimiter">'</span><span class="content">TokenId</span><span class="delimiter">'</span></span>], sent[<span class="string"><span class="delimiter">'</span><span class="content">Token</span><span class="delimiter">'</span></span>], sent[<span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>]):  <span class="comment"># go through each token in the sentence</span>

        print(tok_id, tok, pos)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_api">5.4. API</h3>
<div class="paragraph">
<p>In addition to the examples above, an API (application program
interface) will be provided, containing helper functions that simplify
the retrieval of (combinations of) of features. </p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_example_recipe_calculate_readability_measures_in_r">6. Example Recipe: Calculate Readability Measures in R</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Extracting certain linguistic metrics using the output format of the NLP pipeline as a data frame in R or Python Pandas works straight forward. The following recipe is mainly aimed at demonstrating how to access, address, and use data in an R data frame. As already shown above, the output file can be loaded into the R environment with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">df = read.table(&quot;PathToFile&quot;, header = T, fill = T)</code></pre>
</div>
</div>
<div class="paragraph">
<p>To compute, for example, the <strong>type token ratio</strong> (TTR) of the text, we take the column containing the tokens that can be addressed as 'df$Token'. We remove the punctuation by subsetting that vector formulating a logical condition that refers to the column containing the part-of-speech tags (df$Token[df$CPOS != "PUNC"]). The function unique() and length() allow us to generate a vector of unique types, and to measure the lengths of vectors.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">types = length(unique(df$Token[df$CPOS != &quot;PUNC&quot;]))
tokens = length(df$Token[df$CPOS != &quot;PUNC&quot;])
TTR = types / tokens</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, that we have computed the TTR, we can advance to slightly more complicated calculations in the same manner. Readability measures, a widely used class of linguistic metrics, are a simple means to estimate the difficulty of reading a text, e.g. to choose a suitable text for a reading exercise at school. We can easily calculate the such measures too. In this recipe we want to calculate both the so-called 'Automated Readability Index' or <strong>ARI</strong> and the <strong>LIX</strong> readability index from the output data frame in R. The ARI is calculated from the number of characters, the number of words, and the number of sentences. For computing the LIX we need the number of words, the number of periods, and the number of words longer than six characters.</p>
</div>
<div class="paragraph">
<p>The easiest step is to extract the <strong>number of sentences</strong>: The only thing you need to do is to find the highest sentence ID number using the function max() on the column containing the sentence IDs (df$sentenceId).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">sentences = max(df$SentenceId, na.rm = T)</code></pre>
</div>
</div>
<div class="paragraph">
<p>To compute the <strong>number of words</strong>, we simply take the length of the Token column in the data frame (df$Token), again excluding all entries the POS-tagger has identified as punctuation symbols.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">words = length(df$Token[df$CPOS != &quot;PUNC&quot;])</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <strong>number of periods</strong> requires a somewhat more complicated excluding condition. Our POS tag set only marks punctuation in general, the LIX Readability Index specifically defines full stop period, colon, exclamation mark and question mark as periods. Hence, we want to exclude comma and semicolon from the selection. We will once more rely on the function length() to count elements. This time, we want to count only the elements tagged as punctuation (dfCPOS == "PUNC") and to exclude commas and semicolons.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">periods = length(df$CPOS[df$CPOS == &quot;PUNC&quot; &amp; df$Token != &quot;,&quot; &amp; df$Token != &quot;;&quot;])</code></pre>
</div>
</div>
<div class="paragraph">
<p>To calculate the remaining features, we begin by counting the <strong>characters in each word</strong>. The words themselves can be found in the column 'df$Token'. The function nchar() counts the characters in a string. The function lapply() can be used to apply nchar() upon each single element of df$Token. nchar() Can only be applied on character strings. To ensure that df$Token is of that type and has not accidentally interpreted as a factor when the data frame was loaded, we use the function as.character(), that can transform a factor into a vector of strings.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">word_length = lapply(as.character(df$Token), nchar)</code></pre>
</div>
</div>
<div class="paragraph">
<p>As lapply() returns a list, we must convert the results into vector format (with unlist()), then we can get rid of the punctuation tokens.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">word_length = unlist(word_length)
word_length = word_length[df$CPOS != &quot;PUNC&quot;]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now that we have a vector at hand that contains the length of every single word in the text as a number, we can simply sum it up to calculate <strong>text length in characters</strong>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">characters = sum(word_length)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And we can now compute the number <strong>long words</strong>, i.e. the number of words longer than six characters.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">longwords = length(word_length[word_length &gt; 6])</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now all necessary features have been computed and stored in variables. To <strong>calculate the ARI</strong> for the text, we just need to put the feature values into the ARI formula,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">ARI = 4.71 * (characters / words) + 0.5 * (words / sentences) - 21.43</code></pre>
</div>
</div>
<div class="paragraph">
<p>and into another formula for <strong>calculating the LIX</strong>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">LIX = (words / periods) + (100 * longwords / words)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_example_recipe_stylometric_analysis_with_the_stylo_package_in_r">7. Example Recipe: Stylometric Analysis with the "Stylo" Package in R</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this recipe, we will demonstrate how to use the NLP pipeline&#8217;s output
to explore different stylometrical aspects in a set of example texts
using Stylo.
The <strong><a href="https://sites.google.com/site/computationalstylistics/stylo">Stylo</a></strong>
package is a popular tool written in R that provides a graphical
interface to several functions for stylometrical analysis. Usually,
Stylo takes a folder containing plain or xml text files as input. The
user is then free to choose among different stylometrical procedures,
e.g. PCA, and Burrows' Delta, and different kinds of features to
analyze. Currently (in June 2015) available features are single words,
word n-grams and character n-grams. In this recipe, it will be
demonstrated how to use the output of our NLP pipeline to build
sophisticated features for analysis in Stylo. In this example, two
different feature types will replace the original words of the texts:
the descriptive vocabulary, i.e. the adjectives and adverbs, and the
abstract sentence structures in terms of n-grams of part-of-speech
tags. </p>
</div>
<div class="sect2">
<h3 id="_example_corpus">7.1. Example Corpus</h3>
<div class="paragraph">
<p>The
<a href="https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/raw/master/doc/content/beispielkorpus-kurzgeschichten.zip">example
set</a> is a small collection of English short stories (the "small" and
"short" aspects hopefully improving processing time in a way suitable
for an example tutorial) written between 1889 and 1936 by four different
authors: Rudyard Kipling, Arthur Conan Doyle, H. P. Lovecraft and Robert
E. Howard. The texts are all public domain and available
on <a href="https://www.gutenberg.org/">Project Gutenberg</a>, headers and metadata
were removed from the plain text files before processing.</p>
</div>
</div>
<div class="sect2">
<h3 id="_preparing_descriptive_vocabulary_and_part_of_speech_tags">7.2. Preparing Descriptive Vocabulary and Part-of-Speech Tags</h3>
<div class="paragraph">
<p>After running the NLP processing pipeline, the next step is to read out
the relevant information from the CSV-files and store it in a form
digestible for Stylo. Stylo processes input files from a folder named
"corpus" in the working directory located within the current working
directory.</p>
</div>
<div class="paragraph">
<p>The first thing to do is to set R&#8217;s <strong>working directory</strong> to your current
working folder, i.e. the one where the CSV files are to be found. In R,
the working directory can be changed using the "setwd()" command in the
R console, like in</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">setwd(&quot;~/DKPro/&quot;)</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are uncertain about your current working directory, you can
compute it by typing</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">getwd()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following R-code will <strong>extract the desired features</strong> from the
CSV-files and store them in a Stylo-accessible way.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r"># Extract file names
files = list.files(pattern = &quot;*.csv&quot;)
 
# Create directories
dir.create(&quot;dv/&quot;)
dir.create(&quot;pos/&quot;)
dir.create(&quot;dv/corpus/&quot;)
dir.create(&quot;pos/corpus/&quot;)
 
for(file in files)
{
  # Read file
  df = read.table(file, header = T, fill = T)
 
  # Prepare filename
  shortfile = sub(&quot;.csv&quot;, &quot;&quot;, file)
 
  # Write Adjectives and Adverbes to analyse the author's inventaar of descriptive vocabulary
  dv = df$Lemma[df$CPOS == &quot;ADJ&quot; | df$CPOS == &quot;ADV&quot;]
  filename = paste(&quot;./dv/corpus/&quot;, shortfile, sep = &quot;&quot;)
  write(paste(dv, collapse = &quot; &quot;), file = filename)
 
  # Write POS tags to compare sentence structure
  filename = paste(&quot;./pos/corpus/&quot;, shortfile, sep = &quot;&quot;)
  write(paste(df$CPOS, collapse=&quot; &quot;), file = filename)
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_using_stylo">7.3. Using Stylo</h3>
<div class="paragraph">
<p>If you have not <strong>installed</strong> the Stylo package yet, do that with the
following command into the R console:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">install.packages(&quot;stylo&quot;)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, you can <strong>load the package</strong> with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">library(stylo)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The workflow requires you at this point to decide on the particular
analysis, either the descriptive vocabulary or the part-of-speech tag,
you intend to start with. As Stylo only accepts a single "corpus" folder
as input, you will have to do these separately. The order, however,
depends on your preference (or curiosity) only. If you want to analyze
the <strong>descriptive vocabulary</strong>, type:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">setwd(&quot;./dv/&quot;)</code></pre>
</div>
</div>
<div class="paragraph">
<p>For working with <strong>part-of-speech tags</strong>, type:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">setwd(&quot;./pos/&quot;)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once one of the folders is chosen, you can <strong>start Stylo</strong> by typing</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="r">stylo()</code></pre>
</div>
</div>
<div class="paragraph">
<p>into the R console. The interface will appear:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/stylo.png" alt="Stylo"></span></p>
</div>
<div class="paragraph">
<p>You can now, for example, run a cluster analysis in Stylo. Doing that
with the <strong>unprocessed texts</strong>, yields the following result:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/unprocessed_cluster.png" alt="Cluster"></span></p>
</div>
<div class="paragraph">
<p>The authors are clearly separated, the British authors Doyle and Kipling
are grouped together on one branch, the two Americans on the other.</p>
</div>
<div class="paragraph">
<p>Now, you can change into the folder with the <strong>descriptive vocabulary</strong>,
and try the same procedure. With the example data set, we get the
following result:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/descriptive_cluster.png" alt="Cluster"></span></p>
</div>
<div class="paragraph">
<p>While text from the same authors still clustering together, it seems
that, in contrary to their overall stylistic profile, Howard and Kipling
are more similar to each other, than to the other investigated writers
in terms of their preferred use of adjectives and adverbs.</p>
</div>
<div class="paragraph">
<p>Now, when changing into the folder containing the <strong>part-of-speech</strong> tags,
it is important for gaining useful results to go to the "Features" tab
in the Stylo interface and choose n-grams instead of single words as
features. Our example data set, yields the following output, when using
trigrams as features:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/pos_cluster.png" alt="image"></span></p>
</div>
<div class="paragraph">
<p>Interpreting the frequency trigrams of part-of-speech tags an
approximation for the preference of certain sentence structures, three
of the authors in the test set appear to be quite consistent in their
individual syntax preferences, whereas the three texts the from Rudyard
Kipling in our sample display a remarkable variability.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="TopicModelinginPython">8. Example Recipe: Topic Modeling in Python</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Topic modeling refers to a family of computational techniques that can
be used to discover the main themes in a set of texts by statistically
analyzing patterns of word usage. The term is often used synonymously
with <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA</a> (see
Blei&#8217;s
<a href="https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf">introductory
paper</a>), which is also the variant we will be working with in this
tutorial. There have been written numerous introductions to topic
modeling for humanists (e.g. <a href="https://de.dariah.eu/tatom/index.html">[1]</a>
<a href="http://programminghistorian.org/lessons/topic-modeling-and-mallet">[2]</a> <a href="http://mcburton.net/blog/joy-of-tm">[3]</a>), which provide another level of
detail regarding its technical and epistemic properties. Here it should
just be pointed out that it is a
<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words</a> approach
purely based on word frequencies, which is unsupervised (it doesn&#8217;t have
to be trained on any domain-specific dataset) and thus also works with
literary and historical texts out of the box. However, as the algorithm
was devised with summarizing news articles and other short text types in
mind, its functioning is rather sensitive to text length. Also,
depending on the research question, a rigorous selection process has
shown to be fruitful, e.g. if you are not explicitly looking for the
appearance of literary characters in certain semantic contexts, topics
may become more informative when named entities are being excluded from
the model.</p>
</div>
<div class="paragraph">
<p>We are using the <a href="https://radimrehurek.com/gensim">Gensim</a> package for
Python (make sure <strong>v0.12.4</strong> or higher is installed), but of course there are other well known LDA implementations,
notably <a href="http://mallet.cs.umass.edu/">Mallett</a> for Java
and <a href="http://cran.r-project.org/web/packages/topicmodels/index.html">topicmodels</a> for
R. </p>
</div>
<div class="paragraph">
<p>You can find the complete, ready-to-run scripts for this recipe
<a href="https://github.com/stefanpernes/dariah-nlp-tutorial">here</a>.</p>
</div>
<div class="sect2">
<h3 id="_example_corpus_2">8.1. Example Corpus</h3>
<div class="paragraph">
<p>Any plain text or collection of texts can be used as input for topic
modeling, however, this recipe is based on the pipeline&#8217;s CSV output for
an improved feature selection process, e.g. controlling what should be
included or excluded from the model. We will use the
same <a href="https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/raw/master/doc/content/beispielkorpus-kurzgeschichten.zip">collection
of English short stories</a> as in the last recipe, featuring works by
Rudyard Kipling, Arthur Conan Doyle, H. P. Lovecraft, and Robert E.
Howard. </p>
</div>
</div>
<div class="sect2">
<h3 id="_setting_up_the_environment">8.2. Setting Up the Environment</h3>
<div class="paragraph">
<p>The following code is designed to run with Python 3, which is
recommended for its
built-in <a href="https://en.wikipedia.org/wiki/Unicode">Unicode</a> capabilities and
various other improvements. Assuming that you have Python (and its
package manager <em>pip</em>) installed, issuing the following command at the
command line will download and install the packages needed for this
recipe:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">pip3 install gensim pandas numpy pyLDAvis</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Note:</strong> In case pip install produces an error, try its predecessor <em>easy_install</em>. Not recommended on OS X, though, as the command defaults to the 2.7 Python installation that is shipped with OS X.
<strong>Note:</strong> pyLDAvis is currently not available under Windows (as of
02/2016)</p>
</div>
<div class="paragraph">
<p>Also needed for this recipe is the widely used visualization
package <em>matplotlib</em> (at least <strong>v1.5.1</strong> or higher) for which installation directions are a bit
different on each platform. If you are on a Debian based Linux system
such as Ubuntu, you can use</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">sudo apt-get install python-matplotlib</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are on OS X you can just use <em>pip</em></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">pip3 install matplotlib</code></pre>
</div>
</div>
<div class="paragraph">
<p>For installation on Windows (and other Linux systems), please have a
look at matplotlib&#8217;s
<a href="http://matplotlib.org/users/installing.html">official documentation</a>.</p>
</div>
<div class="paragraph">
<p>Now, for actually running this recipe, the most simplistic way would be
to just start <em>python</em> and enter the code line by line, but it is highly
recommended to look into
<a href="http://ipython.org/notebook.html">IPython/Jupyter</a> notebooks, if you like
to work interactively. Most of the time however, you will want to put
the code into a text file and make it a script that can be interpreted
by Python. When naming the script, use the file extension <em>.py</em> - e.g.
<em>lda.py</em> - and enter the following as its first line:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment">#!/usr/bin/env python</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This takes care of finding the Python interpreter. Furthermore, on Unix
systems the script needs to be made executable by typing <em>chmod +x
lda.py</em> on the command line. On Windows systems everything should be
handled automatically as of Python version 3.3.</p>
</div>
<div class="paragraph">
<p>If the following statements run without error, everything is installed
correctly:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">gensim.corpora</span> <span class="keyword">import</span> <span class="include">MmCorpus</span>, <span class="include">Dictionary</span>
<span class="keyword">from</span> <span class="include">gensim.models</span> <span class="keyword">import</span> <span class="include">LdaMulticore</span>
<span class="keyword">import</span> <span class="include">pandas</span> <span class="keyword">as</span> pd
<span class="keyword">import</span> <span class="include">numpy</span> <span class="keyword">as</span> np
<span class="keyword">import</span> <span class="include">os</span>
<span class="keyword">import</span> <span class="include">sys</span>
<span class="keyword">import</span> <span class="include">csv</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>These should be placed right after the first line, or, when working
interactively, they are the first lines of the script.</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> The model specified here is its parallelized version that uses
all CPU cores to speed up training. For the single core version, just
replace 'LdaMulticore' with 'LdaModel'.</p>
</div>
</div>
<div class="sect2">
<h3 id="_configuration_section">8.3. Configuration Section</h3>
<div class="paragraph">
<p>The following statements are so called 'constants' that reside in the
global variable space of the script, being accessible to all functions
and other sub-entities. This can be viewed as a configuration section,
which we will use to set parameters for pre-processing and modeling.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># input</span>
columns = [<span class="string"><span class="delimiter">'</span><span class="content">ParagraphId</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">TokenId</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">NamedEntity</span><span class="delimiter">'</span></span>]   <span class="comment"># columns to read from csv file</span>
pos_tags = [<span class="string"><span class="delimiter">'</span><span class="content">ADJ</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">NN</span><span class="delimiter">'</span></span>]                        <span class="comment"># parts-of-speech to include into the model</span>
<span class="error"> </span>
<span class="comment"># stopwords</span>
stopwordlist = <span class="string"><span class="delimiter">&quot;</span><span class="content">stopwords.txt</span><span class="delimiter">&quot;</span></span>                  <span class="comment"># path to text file, e.g. stopwords.txt in the same directory as the script</span>
<span class="error"> </span>
<span class="comment"># document size (in words)</span>
<span class="comment">#doc_size = 1000000 # set to arbitrarily large value to use original doc size</span>
doc_size = <span class="integer">1000</span>                                 <span class="comment"># the document size for LDA commonly ranges from 500-2000 words</span>
doc_split = <span class="integer">0</span>                                   <span class="comment"># uses the pipeline's ParagraphId to split text into documents, overrides doc_size - 1: on, 0: off </span>
<span class="error"> </span>
<span class="comment"># model parameters, cf. https://radimrehurek.com/gensim/models/ldamodel.html</span>
no_of_topics = <span class="integer">20</span>                               <span class="comment"># no. of topics to be generated</span>
no_of_passes = <span class="integer">200</span>                              <span class="comment"># no. of lda iterations - usually, the more the better, but increases computing time</span>
<span class="error"> </span>
eval = <span class="integer">1</span>                                        <span class="comment"># perplexity estimation every n chunks - the smaller the better, but increases computing time</span>
chunk = <span class="integer">10</span>                                      <span class="comment"># documents to process at once</span>
<span class="error"> </span>
alpha = <span class="string"><span class="delimiter">&quot;</span><span class="content">symmetric</span><span class="delimiter">&quot;</span></span> <span class="comment"># &quot;symmetric&quot;, &quot;asymmetric&quot;, &quot;auto&quot;, or array (default: a symmetric 1.0/num_topics prior)</span>
                                                <span class="comment"># affects sparsity of the document-topic (theta) distribution</span>


<span class="comment"># custom alpha may increase topic coherence, but may also produce more topics with zero probability</span>
<span class="comment">#alpha = np.array([ 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.05,</span>
<span class="comment"># 0.05, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02])</span>

eta = <span class="predefined-constant">None</span>                                      <span class="comment"># can be a number (int/float), an array, or None</span>
                                                <span class="comment"># affects topic-word (lambda) distribution - not necessarily beneficial to topic coherence</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Note:</strong> Here, we are using the CPOS column, which takes its values
from DKPro&#8217;s universal coarse-grained tag set (consisting of 13
tags: <em>ADJ, ADV, ART, CARD, CONJ, N (NP, NN), O, PP, PR, V, PUNC</em>).
Alternatively, you can always use the POS column for a more fine grained
selection. Currently the pipeline
includes <a href="https://code.google.com/p/mate-tools">MatePosTagger</a>, which
produces output based on e.g.
the <a href="http://www.clips.ua.ac.be/pages/mbsp-tags">Penn Tree Bank</a> tag set
for English
and <a href="http://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/TagSets/stts-table.html">STTS</a> for
German. More information about DKPro components and the tag sets they
are trained on can be
found <a href="https://dkpro.github.io/dkpro-core/releases/1.7.0/models/">here</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_preparing_the_data">8.4. Preparing the Data</h3>
<div class="paragraph">
<p>As in many other machine learning applications, the amount of code
needed to clean the data and to bring it into a form that can be
processed far exceeds the actual modeling code (when using some kind of
framework as it is the case here). What keeps the following code rather
short, are the properties of the pipeline output format which make it
easy to filter for feature combinations. As noted before - although in
principle topic modeling works with completely unrestricted text - we
want to be able to select certain word forms (based on their POS-tags)
and match other restrictions (e.g. not to include named entities).
Another thing we want to control is the size of text segments that get
passed over to LDA as "documents" - as you experiment with different
sizes you will notice that documents which are too large (novels as a
whole) or too small (short scenes) both produce rather meaningless
topics. A document size between 500 - 2000 words should yield acceptable
results. Apart from producing arbitrary text segments of fixed size, we
can also use the pipeline&#8217;s ParagraphId feature, which can be set to
count paragraphs using a string pattern.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">preprocessing</span>(path, columns, pos_tags, doc_size, doc_split, stopwordlist):
    docs = []
    doc_labels = []
    stopwords = <span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>
<span class="error"> </span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">reading files ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
    <span class="keyword">try</span>:
        <span class="keyword">with</span> <span class="predefined">open</span>(stopwordlist, <span class="string"><span class="delimiter">'</span><span class="content">r</span><span class="delimiter">'</span></span>) <span class="keyword">as</span> f: stopwords = f.read()
    <span class="keyword">except</span> <span class="exception">OSError</span>:
        <span class="keyword">pass</span>
    stopwords = <span class="predefined">sorted</span>(<span class="predefined">set</span>(stopwords.split(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)))
<span class="error"> </span>
    <span class="keyword">for</span> <span class="predefined">file</span> <span class="keyword">in</span> os.listdir(path=path):
        <span class="keyword">if</span> <span class="keyword">not</span> <span class="predefined">file</span>.startswith(<span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>):
            filepath = path+<span class="string"><span class="delimiter">&quot;</span><span class="content">/</span><span class="delimiter">&quot;</span></span>+<span class="predefined">file</span>
            print(filepath)
<span class="error"> </span>
            df = pd.read_csv(filepath, sep=<span class="string"><span class="delimiter">&quot;</span><span class="char">\t</span><span class="delimiter">&quot;</span></span>, quoting=csv.QUOTE_NONE)
            df = df[columns]
            df = df.groupby(<span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>)
<span class="error"> </span>
            doc = pd.DataFrame()
            <span class="keyword">for</span> p <span class="keyword">in</span> pos_tags:                          <span class="comment"># collect only the specified parts-of-speech</span>
                doc = doc.append(df.get_group(p))
<span class="error"> </span>
            names = df.get_group(<span class="string"><span class="delimiter">'</span><span class="content">NP</span><span class="delimiter">'</span></span>)[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>)  <span class="comment"># add proper nouns to stopword list</span>
            stopwords += names.tolist()
<span class="error"> </span>
            <span class="comment"># construct documents</span>
            <span class="keyword">if</span> doc_split:                               <span class="comment"># size according to paragraph id</span>
                doc = doc.groupby(<span class="string"><span class="delimiter">'</span><span class="content">ParagraphId</span><span class="delimiter">'</span></span>)
                <span class="keyword">for</span> para_id, para <span class="keyword">in</span> doc:
                    docs.append(para[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>))
                    doc_labels.append(<span class="predefined">file</span>.split(<span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>)[<span class="integer">0</span>]+<span class="string"><span class="delimiter">&quot;</span><span class="content"> #</span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(para_id))     <span class="comment"># use filename + doc id as plot label</span>
            <span class="keyword">else</span>:                                       <span class="comment"># size according to doc_size</span>
                doc = doc.sort(columns=<span class="string"><span class="delimiter">'</span><span class="content">TokenId</span><span class="delimiter">'</span></span>)
                i = <span class="integer">1</span>
                <span class="keyword">while</span>(doc_size &lt; doc.shape[<span class="integer">0</span>]):
                    docs.append(doc[:doc_size][<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>))
                    doc_labels.append(<span class="predefined">file</span>.split(<span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>)[<span class="integer">0</span>]+<span class="string"><span class="delimiter">&quot;</span><span class="content"> #</span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(i))
                    doc = doc.drop(doc.index[:doc_size])        <span class="comment"># drop doc_size rows</span>
                    i += <span class="integer">1</span>
                docs.append(doc[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>))    <span class="comment"># add the rest</span>
                doc_labels.append(<span class="predefined">file</span>.split(<span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>)[<span class="integer">0</span>]+<span class="string"><span class="delimiter">&quot;</span><span class="content"> #</span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(i))
<span class="error"> </span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">normalizing and vectorizing ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)        <span class="comment"># cf. https://radimrehurek.com/gensim/tut1.html</span>
<span class="error"> </span>
    texts = [[word <span class="keyword">for</span> word <span class="keyword">in</span> doc <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords] <span class="keyword">for</span> doc <span class="keyword">in</span> docs]       <span class="comment"># remove stopwords</span>
<span class="error"> </span>
    all_tokens = <span class="predefined">sum</span>(texts, [])                                                     <span class="comment"># remove words that appear only once</span>
    tokens_once = <span class="predefined">set</span>(word <span class="keyword">for</span> word <span class="keyword">in</span> <span class="predefined">set</span>(all_tokens) <span class="keyword">if</span> all_tokens.count(word) == <span class="integer">1</span>)
    texts = [[word <span class="keyword">for</span> word <span class="keyword">in</span> text <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> tokens_once] <span class="keyword">for</span> text <span class="keyword">in</span> texts]
<span class="error"> </span>
    dictionary = Dictionary(texts)                      <span class="comment"># vectorize</span>
    corpus = [dictionary.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> texts]
<span class="error"> </span>
    <span class="keyword">return</span> dictionary, corpus, doc_labels</code></pre>
</div>
</div>
<div class="paragraph">
<p>It might be the case that filtering out named entities using information
from the NamedEntity column still leaves too many unwanted names in the
model. That can happen because NER components differ in performance for
different languages and different types of text. An independently
developed NER component trained on German 19th century novels will be
included in a later version of the pipeline to address use cases like
this. The following lines will add all named entities to the stopword list.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">df = df.groupby(<span class="string"><span class="delimiter">'</span><span class="content">NamedEntity</span><span class="delimiter">'</span></span>)

names = df.get_group(<span class="string"><span class="delimiter">'</span><span class="content">B-PER</span><span class="delimiter">'</span></span>)[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>)

names += df.get_group(<span class="string"><span class="delimiter">'</span><span class="content">I-PER</span><span class="delimiter">'</span></span>)[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>)

stopwords += names.tolist()</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the meanwhile, and as a more generic approach, we filter
out all proper nouns (NP).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">df = df.groupby(<span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>)

names = df.get_group(<span class="string"><span class="delimiter">'</span><span class="content">NP</span><span class="delimiter">'</span></span>)[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>].values.astype(<span class="predefined">str</span>)

stopwords += names.tolist()</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_fitting_the_model">8.5. Fitting the Model</h3>
<div class="paragraph">
<p>Next, we can put it all together. The following is the script&#8217;s entry
point, which is usually placed at the bottom of every Python script. It
checks for a command line argument, which should be a path. That path
gets handed over to the preprocessing() function, which loads file after
file and performs feature selection as well as vectorization of the
data. The resulting dictionary and corpus objects are then used to
create a LdaMulticore() model. Afterwards, the topics are displayed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">if</span> <span class="predefined">len</span>(sys.argv) &lt; <span class="integer">2</span>:
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">usage: {0} [folder containing csv files]</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
          <span class="string"><span class="delimiter">&quot;</span><span class="content">parameters are set inside the script.</span><span class="delimiter">&quot;</span></span>.format(sys.argv[<span class="integer">0</span>]))
    sys.exit(<span class="integer">1</span>)
<span class="error"> </span>
path = sys.argv[<span class="integer">1</span>]
foldername = path.split(<span class="string"><span class="delimiter">&quot;</span><span class="content">/</span><span class="delimiter">&quot;</span></span>)[-<span class="integer">1</span>]
<span class="error"> </span>
dictionary, corpus, doc_labels = preprocessing(path, columns, pos_tags, doc_size, doc_split, stopwordlist)

print(<span class="string"><span class="delimiter">&quot;</span><span class="content">fitting the model ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=no_of_topics, passes=no_of_passes,
                 eval_every=<span class="predefined">eval</span>, chunksize=chunk, alpha=alpha, eta=eta)
<span class="error"> </span>
print(model, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
topics = model.show_topics(num_topics=no_of_topics)
<span class="error"> </span>
<span class="keyword">for</span> item, i <span class="keyword">in</span> <span class="predefined">zip</span>(topics, <span class="predefined">enumerate</span>(topics)):
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">topic #</span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(i[<span class="integer">0</span>])+<span class="string"><span class="delimiter">&quot;</span><span class="content">: </span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(item)+<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p>For the example corpus this produces the following topics (shows the top
10 terms for each topic, the order of topics is random by default):</p>
</div>
<div class="listingblock">
<div class="content">
<pre>topic #0: 0.012*instant + 0.011*universe + 0.010*mad + 0.008*way + 0.008*everyone + 0.007*ship + 0.007*whilst + 0.007*other + 0.007*poor + 0.007*moment
topic #1: 0.008*world + 0.007*horror + 0.006*years + 0.006*body + 0.006*other + 0.006*terrible + 0.004*woman + 0.004*tree + 0.004*family + 0.004*baronet
topic #2: 0.009*corridor + 0.009*foot + 0.009*hand + 0.008*woman + 0.007*eyes + 0.007*lover + 0.007*floor + 0.006*chamber + 0.006*shape + 0.006*estate
topic #3: 0.012*point + 0.012*foot + 0.011*specimen + 0.011*inch + 0.009*print + 0.008*tube + 0.008*vegetable + 0.008*animal + 0.008*camp + 0.008*diameter
topic #4: 0.012*other + 0.012*way + 0.012*face + 0.010*case + 0.010*last + 0.010*eyes + 0.009*hand + 0.009*moor + 0.007*nothing + 0.006*anything
topic #5: 0.013*arms + 0.008*shape + 0.006*human + 0.005*tree + 0.005*lip + 0.005*neck + 0.005*face + 0.005*loam + 0.005*pave + 0.005*preferable
topic #6: 0.000*incoherent + 0.000*reality + 0.000*riches + 0.000*fearful + 0.000*neighbor + 0.000*oriental + 0.000*liking + 0.000*tentacle + 0.000*prize-fighter + 0.000*bristle
topic #7: 0.016*eyes + 0.012*poor + 0.011*anything + 0.010*hot + 0.009*punkah + 0.009*chap + 0.009*cooly + 0.008*face + 0.008*native + 0.006*sort
topic #8: 0.017*stain + 0.015*chemical + 0.012*test + 0.009*file + 0.009*rooms + 0.008*wagonette + 0.007*text + 0.007*eccentric + 0.007*fare + 0.006*misfortune
topic #9: 0.017*buffalo + 0.016*foot + 0.015*child + 0.015*herd + 0.014*things + 0.013*branch + 0.011*boy + 0.010*eyes + 0.010*moon + 0.009*skin
topic #10: 0.000*incoherent + 0.000*reality + 0.000*riches + 0.000*fearful + 0.000*neighbor + 0.000*oriental + 0.000*liking + 0.000*tentacle + 0.000*prize-fighter + 0.000*bristle
topic #11: 0.017*eyes + 0.013*tree + 0.013*foot + 0.009*hand + 0.008*cliff + 0.008*fire + 0.007*hands + 0.007*shoulder + 0.007*figure + 0.007*ruin
topic #12: 0.026*things + 0.020*dretful + 0.017*home + 0.016*while + 0.013*fine + 0.011*legs + 0.010*round + 0.010*afraid + 0.009*loud + 0.008*bit
topic #13: 0.000*incoherent + 0.000*reality + 0.000*riches + 0.000*fearful + 0.000*neighbor + 0.000*oriental + 0.000*liking + 0.000*tentacle + 0.000*prize-fighter + 0.000*bristle
topic #14: 0.013*desert + 0.008*palm + 0.008*human + 0.007*hand + 0.006*hut + 0.006*other + 0.006*lamp + 0.005*shadow + 0.005*eyes + 0.005*foot
topic #15: 0.009*case + 0.009*other + 0.008*family + 0.006*cellar + 0.005*manuscript + 0.005*record + 0.005*account + 0.005*much + 0.005*years + 0.005*interest
topic #16: 0.015*wind + 0.015*plane + 0.013*camp + 0.012*snow + 0.010*wireless + 0.010*world + 0.009*other + 0.009*antarctic + 0.008*whole + 0.008*seal
topic #17: 0.000*incoherent + 0.000*reality + 0.000*riches + 0.000*fearful + 0.000*neighbor + 0.000*oriental + 0.000*liking + 0.000*tentacle + 0.000*prize-fighter + 0.000*bristle
topic #18: 0.011*foot + 0.009*base + 0.008*plane + 0.008*world + 0.008*camp + 0.007*crew + 0.007*trip + 0.007*peak + 0.007*years + 0.006*unknown
topic #19: 0.003*cleanliness + 0.003*hawk-like + 0.003*luncheon + 0.000*readiness + 0.000*channels + 0.000*brigade + 0.000*enthusiast + 0.000*exactness + 0.000*edition + 0.000*politics</pre>
</div>
</div>
<div class="paragraph">
<p>When you put everything together and do a test run, you will notice that
producing an LDA model can take quite some time - if you have a lot of
text to process, that might be something to do over night. Furthermore,
as LDA is a generative and probabilistic model, its output is slightly
different each time it is run (though, with a high number of iterations
- see
<strong><a href="#ConfigurationSection">configuration section</a></strong> - results should be pretty stable).</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> The configuration options implemented and discussed in this
recipe will most likely <strong>have to be adjusted</strong> for use with another set
of texts - be sure to experiment with different numbers of topics,
iterations, document sizes, parts-of-speech to include, and if you&#8217;re
feeling adventurous, also try different settings for the LDA
hyperparameters - <em>alpha</em> and <em>eta</em>.</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> If you want to know more about what&#8217;s happening under the
hood, append the following to the import statements at the beginning of
the file. Beware that Gensim&#8217;s logging produces a lot of detailed
output.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">logging</span>
logging.basicConfig(format=<span class="string"><span class="delimiter">'</span><span class="content">%(asctime)s : %(levelname)s : %(message)s</span><span class="delimiter">'</span></span>, level=logging.INFO)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, you can save calculated models to disk and load them
afterwards, e.g. for experimenting with different visualizations. This
last part of the script saves the model, corpus, and dictionary objects
using Gensim&#8217;s
<a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.save">save()</a>
function, as well as document labels and the topics themselves as text
files.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">print(<span class="string"><span class="delimiter">&quot;</span><span class="content">saving ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
<span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string"><span class="delimiter">&quot;</span><span class="content">out</span><span class="delimiter">&quot;</span></span>): os.makedirs(<span class="string"><span class="delimiter">&quot;</span><span class="content">out</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
<span class="keyword">with</span> <span class="predefined">open</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">out/</span><span class="delimiter">&quot;</span></span>+foldername+<span class="string"><span class="delimiter">&quot;</span><span class="content">_doclabels.txt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">w</span><span class="delimiter">&quot;</span></span>) <span class="keyword">as</span> f:
    <span class="keyword">for</span> item <span class="keyword">in</span> doc_labels: f.write(item+<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
<span class="keyword">with</span> <span class="predefined">open</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">out/</span><span class="delimiter">&quot;</span></span>+foldername+<span class="string"><span class="delimiter">&quot;</span><span class="content">_topics.txt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">w</span><span class="delimiter">&quot;</span></span>) <span class="keyword">as</span> f:
    <span class="keyword">for</span> item, i <span class="keyword">in</span> <span class="predefined">zip</span>(topics, <span class="predefined">enumerate</span>(topics)):
        f.write(<span class="string"><span class="delimiter">&quot;</span><span class="content">topic #</span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(i[<span class="integer">0</span>])+<span class="string"><span class="delimiter">&quot;</span><span class="content">: </span><span class="delimiter">&quot;</span></span>+<span class="predefined">str</span>(item)+<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
dictionary.save(<span class="string"><span class="delimiter">&quot;</span><span class="content">out/</span><span class="delimiter">&quot;</span></span>+foldername+<span class="string"><span class="delimiter">&quot;</span><span class="content">.dict</span><span class="delimiter">&quot;</span></span>)
MmCorpus.serialize(<span class="string"><span class="delimiter">&quot;</span><span class="content">out/</span><span class="delimiter">&quot;</span></span>+foldername+<span class="string"><span class="delimiter">&quot;</span><span class="content">.mm</span><span class="delimiter">&quot;</span></span>, corpus)
model.save(<span class="string"><span class="delimiter">&quot;</span><span class="content">out/</span><span class="delimiter">&quot;</span></span>+foldername+<span class="string"><span class="delimiter">&quot;</span><span class="content">.lda</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_visualization_options">8.6. Visualization Options</h3>
<div class="paragraph">
<p>Each of the following visualizations is generated by its own Python
script that is able to draw on contents and metadata of the LDA model
using the save files generated by <em>lda.py. </em>The scripts expect a path
to the generated model <strong>.lda </strong>file and that it is in the same
directory as the other save files.</p>
</div>
<div class="sect3">
<h4 id="_interactive">8.6.1. Interactive</h4>
<div class="paragraph">
<p><strong>[<a href="https://github.com/stefanpernes/dariah-nlp-tutorial/blob/master/lda_interactive.py">Source</a>]</strong> This
piece of code produces an interactive visualization of what the model
has learned from the data. You can explore our example model by
downloading
<a href="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/kurzgeschichten_interactive.html">this
HTML file</a> and opening it in a browser. The figure in the left column
shows a projection of the inter-topic distances onto two dimensions, the
barchart on the right shows the most useful terms for interpreting
selected topic based on the 'relevance metric' slider. Basically, it
allows for an interactive reranking and thus exploration of all terms
connected to the topic, also those, which the model might have placed at
the bottom. Another thing is that terms can be selected and in turn show
how they are distributed on the map. The visualization package pyLDAvis
has been described in
<a href="http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf">this
paper</a>.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/kurzgeschichten_interactive.png" alt="image"></span></p>
</div>
</div>
<div class="sect3">
<h4 id="_heatmap">8.6.2. Heatmap</h4>
<div class="paragraph">
<p><strong>[<a href="https://github.com/stefanpernes/dariah-nlp-tutorial/blob/master/lda_heatmap.py">Source</a>] </strong>The
heatmap option displays the kind of information that is probably most
useful to literary scholars. Going beyond pure exploration, this
visualization can be used to show thematic developments over a set of
texts as well as a single text, akin to a dynamic topic model. What also
becomes apparent here, is that some topics correlate highly with a
specific author or group of authors, while other topics correlate highly
with a specific text or group of texts. All in all, this displays two of
LDA&#8217;s properties - its use as a distant reading tool that aims to get at
text meaning, and its use as a provider of data that can be further used
in computational analysis, such as document classification or authorship
attribution. To get a feel for this visualization you can try
e.g. building a number of models with varying document size
(see <a href="#ConfigurationSection">configuration
section</a> in <em>lda.py</em>) - smaller document sizes 'zoom in' on the
thematic development inside texts, while larger ones 'zoom out', up
until there is only one row per document to display.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/kurzgeschichten_heatmap.png" alt="image"></span></p>
</div>
</div>
<div class="sect3">
<h4 id="_network">8.6.3. Network</h4>
<div class="paragraph">
<p><strong>[<a href="https://github.com/stefanpernes/dariah-nlp-tutorial/blob/master/lda_network.py">Source</a>]</strong> For
a more artistic presentation of a topic model, consider the following
network graph that can be generated using a snippet from
<a href="http://nbviewer.ipython.org/github/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb">The
Art of Literary Text Analysis</a> by Stéfan Sinclair &amp; Geoffrey Rockwell,
namely the
<a href="http://nbviewer.ipython.org/github/sgsinclair/alta/blob/master/ipynb/TopicModelling.ipynb#Graphing-Topic-Terms">Graphing
Topic Terms</a> function, which produces the following graph:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/kurzgeschichten_network.png" alt="image"></span></p>
</div>
<div class="paragraph">
<p>The graph shows the top 30 terms for each topic. Terms that are only
connected to one topic are placed on the outside, while the terms that
appear in more than one topic distribute themselves on the inside. In
contrast to the interactive map example above, the topography of this
network graph is not based on a distance measure but a product of the
layout algorithm.</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> You might want to try out various settings, depending on how
many nodes you need to fit on the canvas. For this visualization the
settings <strong>k=0.060,</strong> <strong>iterations=30</strong> were passed to the
<strong>nx.spring_layout()</strong> function.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_example_recipe_stylometric_classification_in_python">9. Example Recipe: Stylometric Classification in Python</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this recipe, we will show how to implement a cross-genre stylometric
classification system similar to the one proposed by van Halteren et al.
in
<em><a href="http://www.sfs.uni-tuebingen.de/~hbaayen/publications/VanHalterenEtAlJQL.pdf">New
Machine Learning Methods Demonstrate the Existence of a Human
Stylome</a></em>. In short, the authors propose a set of features and a
classification algorithm based on the idea that everyone&#8217;s individual
language form can be classified in terms of a 'stylome', as much as it
can be for experienced writers. While we employ an ordinary <em>Random
Forest Classifier</em> instead of the author&#8217;s own <em>Weighted Probability
Distribution Voting</em> algorithm, we can show how to build a pairwise
classification system that works genre-independently with an accuracy of
around 0.70 using only the feature set.</p>
</div>
<div class="paragraph">
<p>You can find the complete, ready-to-run Python script
on <a href="https://github.com/stefanpernes/dariah-nlp-tutorial">GitHub</a>.</p>
</div>
<div class="sect2">
<h3 id="_example_corpus_3">9.1. Example Corpus</h3>
<div class="paragraph">
<p>The original corpus used in the paper is controlled for various factors
and designed to make the classification task as hard as possible in
order to substantiate the human stylome hypothesis. It consists of 72
Dutch texts by 8 authors, having roughly the same age and educational
background. And it includes different text types: Each author was asked
to produce three argumentative non-­fiction texts, three descriptive
non-­fiction texts, and three fiction texts, each approximately 1,5
pages long. This led to a corpus controlled for register, genre and
topic of the texts. It is suitable for training 72 models (for each
possible pair of authors, based on eight texts each) and deriving a
combined classification score.</p>
</div>
<div class="paragraph">
<p>Since we don&#8217;t have such a fine tuned corpus at hand, we decided to
recreate part of it using freely available texts from
<a href="http://gutenberg.spiegel.de">Project Gutenberg</a>. The example corpus
provided here, consists of texts by two writers from roughly the same
period, <a href="https://en.wikipedia.org/wiki/Heinrich_von_Kleist">Heinrich von
Kleist</a> (1777–1811) and
<a href="https://en.wikipedia.org/wiki/Franz_Grillparzer">Franz Grillparzer</a>
(1791–1872). As it is the case for the original setup, this collection
includes three prose texts, three plays, and three poems for each
author. The filenames reflect their respective text types (although this
information is not needed for the classification experiment) and
indicate whether a longer text has been truncated ("Anfang").
Additionally, some poems had to be concatenated in order to arrive at a
minimum text length of 300 words (labelled "Gedichte"). You can
<strong><a href="https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/raw/master/doc/content/grillparzer-kleist.zip">get
the example corpus here</a></strong>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_setting_up_the_environment_2">9.2. Setting Up the Environment</h3>
<div class="paragraph">
<p>Assuming you have Python installed, issuing the following command at the
command line will download and install the packages needed for this
recipe:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">pip3 install pandas scikit-learn</code></pre>
</div>
</div>
<div class="paragraph">
<p>Have a look at the <a href="#SettinguptheEnvironment">previous recipe setup</a> for more detailed instructions. Now we can use the
following import statements:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">pandas</span> <span class="keyword">as</span> pd
<span class="keyword">import</span> <span class="include">numpy</span> <span class="keyword">as</span> np
<span class="keyword">import</span> <span class="include">os</span>, <span class="include">sys</span>
<span class="keyword">from</span> <span class="include">collections</span> <span class="keyword">import</span> <span class="include">Counter</span>
<span class="keyword">from</span> <span class="include">sklearn.feature_extraction</span> <span class="keyword">import</span> <span class="include">DictVectorizer</span>
<span class="keyword">from</span> <span class="include">sklearn.preprocessing</span> <span class="keyword">import</span> <span class="include">Imputer</span>
<span class="keyword">from</span> <span class="include">sklearn.ensemble</span> <span class="keyword">import</span> <span class="include">RandomForestClassifier</span>
<span class="keyword">from</span> <span class="include">sklearn.cross_validation</span> <span class="keyword">import</span> <span class="include">cross_val_score</span>, <span class="include">ShuffleSplit</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_feature_selection">9.3. Feature Selection</h3>
<div class="paragraph">
<p>The author&#8217;s approach to measuring a human stylome rests on the idea
that any individual form can be classified as long as one looks for a
large enough number of traits, consisting of both, vocabulary as well as
syntactic features. This is also what the feature set in van Halteren et
al. reflects:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>   1. Current token
   2. Previous token
   3. Next token
   4. Concatenation of the wordclass tags of these three tokens (as
       assigned by an automatic WOTANlite tagger (van Halteren et al.., 2001)
   5. Concatenation of
       a. length of the sentence (in 7 classes: 1, 2, 3, 4, 5-10,11-20 or 21+
           tokens)
       b. position in the sentence (in 3 classes: first three tokens, last
           three tokens, other)
   6. Concatenation of
       a. part of speech of the current token, i.e. the initial part of the
           wordclass tag
       b. frequency of the current token in the text (in 5 classes: 1, 2-5,
           6-10,11-20 or 21+)
       c. number of blocks (consisting of 1/7th of the text) in which the
           current token is found (in 4 classes: 1, 2-3,4-6,7)
       d. distance in sentences to the previous occurrence of the current token
           (in 7 classes: NONE, SAME, 1, 2-3,4-7,8-15,16+)</pre>
</div>
</div>
<div class="paragraph">
<p>Taken as a software specification this should prove a worthy test for
the practicability of the CSV format. It translates into the following
<em>featureselect()</em> function plus smaller functions to help with the
calculation of specified classes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">wordcount</span>(wordlist):
    dict = {}
    <span class="keyword">for</span> word <span class="keyword">in</span> wordlist:
        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> <span class="predefined">dict</span>: <span class="predefined">dict</span>[word] = <span class="integer">1</span>
        <span class="keyword">else</span>: <span class="predefined">dict</span>[word] += <span class="integer">1</span>
    <span class="keyword">return</span> <span class="predefined">dict</span>
<span class="error"> </span>
<span class="keyword">def</span> <span class="function">token_in_textblock</span>(text, token):        <span class="comment"># returns number of blocks (consisting of 1/7th of the text)</span>
    blocks = []                             <span class="comment"># in which the current token is found, in 4 classes: 1, 2-3,4-6,7</span>
    block_size = <span class="predefined">len</span>(text)/<span class="integer">7</span>
    last = no_of_blocks = <span class="integer">0</span>
<span class="error"> </span>
    <span class="keyword">while</span> last &lt; <span class="predefined">len</span>(text):
        blocks.append(text[<span class="predefined">int</span>(last):<span class="predefined">int</span>(last + block_size)])
        last += block_size
<span class="error"> </span>
    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:
        <span class="keyword">if</span> token <span class="keyword">in</span> block: no_of_blocks += <span class="integer">1</span>
<span class="error"> </span>
    <span class="keyword">if</span> no_of_blocks == <span class="integer">1</span>: occur_class = <span class="integer">1</span>
    <span class="keyword">elif</span> <span class="integer">2</span> &lt;= no_of_blocks &lt;= <span class="integer">3</span>: occur_class = <span class="integer">2</span>
    <span class="keyword">elif</span> <span class="integer">4</span> &lt;= no_of_blocks &lt;= <span class="integer">6</span>: occur_class = <span class="integer">3</span>
    <span class="keyword">else</span>: occur_class = <span class="integer">4</span>
<span class="error"> </span>
    <span class="keyword">return</span> occur_class
<span class="error"> </span>
<span class="keyword">def</span> <span class="function">distance_to_previous</span>(curr_tok_id, curr_sent_id, occurrences):
    <span class="comment"># returns distance in sentences to the previous occurrence</span>
    <span class="comment"># of the current token (in 7 classes: NONE, SAME, 1, 2-3,4-7,8-15,16+</span>
<span class="error"> </span>
    occurrences = occurrences.reset_index()                             <span class="comment"># add new index from 0 .. len(occurrences.index)</span>
<span class="error"> </span>
    current_key = occurrences[occurrences[<span class="string"><span class="delimiter">'</span><span class="content">TokenId</span><span class="delimiter">'</span></span>] == curr_tok_id].index[<span class="integer">0</span>]   <span class="comment"># get row corresponding to curr_tok_id + its new index value</span>
<span class="error"> </span>
    <span class="keyword">if</span> current_key &gt; <span class="integer">0</span>:                                                 <span class="comment"># there is more than one &amp;&amp; its not the first occurrence</span>
        prev_sent_id = <span class="predefined">int</span>(occurrences.iloc[current_key-<span class="integer">1</span>, <span class="integer">1</span>])          <span class="comment"># get previous sentence id based on that index</span>
<span class="error"> </span>
        dist = curr_sent_id - prev_sent_id
<span class="error"> </span>
        <span class="keyword">if</span> dist == <span class="integer">0</span>: d_class = <span class="integer">2</span>
        <span class="keyword">elif</span> dist == <span class="integer">1</span>: d_class = <span class="integer">3</span>
        <span class="keyword">elif</span> <span class="integer">2</span> &lt;= dist &lt;= <span class="integer">3</span>: d_class = <span class="integer">4</span>
        <span class="keyword">elif</span> <span class="integer">4</span> &lt;= dist &lt;= <span class="integer">7</span>: d_class = <span class="integer">5</span>
        <span class="keyword">elif</span> <span class="integer">8</span> &lt;= dist &lt;= <span class="integer">15</span>: d_class = <span class="integer">6</span>
        <span class="keyword">elif</span> <span class="integer">16</span> &lt;= dist: d_class = <span class="integer">7</span>
    <span class="keyword">else</span>:
        d_class = <span class="integer">1</span>
<span class="error"> </span>
    <span class="keyword">return</span> d_class
<span class="error"> </span>


<span class="keyword">def</span> <span class="function">featureselect</span>(text):
    columns = [<span class="string"><span class="delimiter">'</span><span class="content">SentenceId</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">TokenId</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">Token</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>]
    columns_features = [<span class="string"><span class="delimiter">'</span><span class="content">CurrToken</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">PrevToken</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">NextToken</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">TokenTags</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">LengthPosition</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">TagFreqOccur</span><span class="delimiter">'</span></span>]
<span class="error"> </span>
    csv = pd.read_csv(text, sep=<span class="string"><span class="delimiter">&quot;</span><span class="char">\t</span><span class="delimiter">&quot;</span></span>)
    df = csv[columns]                               <span class="comment"># create copy containing only the specified columns</span>
<span class="error"> </span>
    sent_max = df[<span class="string"><span class="delimiter">&quot;</span><span class="content">SentenceId</span><span class="delimiter">&quot;</span></span>].max()               <span class="comment"># number of sentences in the text</span>
    token_max = df[<span class="string"><span class="delimiter">&quot;</span><span class="content">TokenId</span><span class="delimiter">&quot;</span></span>].max()                 <span class="comment"># number of tokens in the text</span>
<span class="error"> </span>
    text = <span class="predefined">list</span>(df[<span class="string"><span class="delimiter">&quot;</span><span class="content">Token</span><span class="delimiter">&quot;</span></span>])
    word_freq = wordcount(text)                     <span class="comment"># word frequencies</span>
<span class="error"> </span>
    features = pd.DataFrame(columns=columns_features, index=<span class="predefined">range</span>(token_max+<span class="integer">1</span>))       <span class="comment"># dataframe to hold the results</span>
<span class="error"> </span>
    <span class="keyword">for</span> sent_id <span class="keyword">in</span> <span class="predefined">range</span>(sent_max+<span class="integer">1</span>):               <span class="comment"># iterate through sentences</span>
        sentence = df[df[<span class="string"><span class="delimiter">'</span><span class="content">SentenceId</span><span class="delimiter">'</span></span>] == sent_id]  <span class="comment"># return rows corresponding to sent_id</span>
<span class="error"> </span>
        s_len = <span class="predefined">len</span>(sentence.index)                 <span class="comment"># length of the sentence</span>
        <span class="keyword">if</span> s_len == <span class="integer">1</span>: s_class = <span class="integer">1</span>                  <span class="comment"># in 7 classes: 1, 2, 3, 4, 5-10,11-20 or 21+ tokens</span>
        <span class="keyword">elif</span> s_len == <span class="integer">2</span>: s_class = <span class="integer">2</span>
        <span class="keyword">elif</span> s_len == <span class="integer">3</span>: s_class = <span class="integer">3</span>
        <span class="keyword">elif</span> s_len == <span class="integer">4</span>: s_class = <span class="integer">4</span>
        <span class="keyword">elif</span> <span class="integer">5</span> &lt;= s_len &lt;= <span class="integer">10</span>: s_class = <span class="integer">5</span>
        <span class="keyword">elif</span> <span class="integer">11</span> &lt;= s_len &lt;= <span class="integer">20</span>: s_class = <span class="integer">6</span>
        <span class="keyword">elif</span> <span class="integer">21</span> &lt;= s_len: s_class = <span class="integer">7</span>
<span class="error"> </span>
        tok_count = <span class="integer">1</span>
        <span class="keyword">for</span> row <span class="keyword">in</span> sentence.iterrows():
            tok_id = row[<span class="integer">0</span>]                         <span class="comment"># row/dataframe index is the same as TokenId</span>
<span class="error"> </span>
            features.iat[tok_id, <span class="integer">0</span>] = current_tok = row[<span class="integer">1</span>].get(<span class="string"><span class="delimiter">&quot;</span><span class="content">Token</span><span class="delimiter">&quot;</span></span>)             <span class="comment"># save current token</span>
            tokentags = current_pos = row[<span class="integer">1</span>].get(<span class="string"><span class="delimiter">&quot;</span><span class="content">CPOS</span><span class="delimiter">&quot;</span></span>)                            <span class="comment"># get current pos tag</span>
<span class="error"> </span>
            <span class="keyword">if</span> tok_id &gt; <span class="integer">0</span>:
                features.iat[tok_id, <span class="integer">1</span>] = df.iloc[tok_id-<span class="integer">1</span>, <span class="integer">2</span>]                      <span class="comment"># save previous token</span>
                tokentags += <span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span> + df.iloc[tok_id-<span class="integer">1</span>, <span class="integer">3</span>]                             <span class="comment"># get previous pos tag</span>
            <span class="keyword">else</span>:
                tokentags += <span class="string"><span class="delimiter">&quot;</span><span class="content">-NaN</span><span class="delimiter">&quot;</span></span>
<span class="error"> </span>
            <span class="keyword">if</span> tok_id &lt; token_max:
                features.iat[tok_id, <span class="integer">2</span>] = df.iloc[tok_id+<span class="integer">1</span>, <span class="integer">2</span>]                      <span class="comment"># save next token</span>
                tokentags += <span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span> + df.iloc[tok_id+<span class="integer">1</span>, <span class="integer">3</span>]                             <span class="comment"># get next pos tag</span>
            <span class="keyword">else</span>:
                tokentags += <span class="string"><span class="delimiter">&quot;</span><span class="content">-NaN</span><span class="delimiter">&quot;</span></span>
<span class="error"> </span>
            features.iat[tok_id, <span class="integer">3</span>] = tokentags                         <span class="comment"># save pos tags</span>
<span class="error"> </span>
            <span class="keyword">if</span> tok_count &lt;= <span class="integer">3</span>: t_class = <span class="integer">1</span>                              <span class="comment"># position in the sentence</span>
            <span class="keyword">elif</span> (s_len-<span class="integer">3</span>) &lt; tok_count &lt;= s_len: t_class = <span class="integer">2</span>            <span class="comment"># in 3 classes: first three tokens, last three tokens, other</span>
            <span class="keyword">else</span>: t_class = <span class="integer">3</span>
<span class="error"> </span>
            features.iat[tok_id, <span class="integer">4</span>] = <span class="predefined">str</span>(s_class) + <span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span> + <span class="predefined">str</span>(t_class) <span class="comment"># save sentence length + token position</span>
<span class="error"> </span>
            tok_freq = word_freq[current_tok]                           <span class="comment"># frequency of the current token in the text</span>
            <span class="keyword">if</span> tok_freq == <span class="integer">1</span>: f_class = <span class="integer">1</span>                               <span class="comment"># in 5 classes: 1, 2-5, 6-10,11-20 or 21+</span>
            <span class="keyword">elif</span> <span class="integer">2</span> &lt;= tok_freq &lt;= <span class="integer">5</span>: f_class = <span class="integer">2</span>
            <span class="keyword">elif</span> <span class="integer">6</span> &lt;= tok_freq &lt;= <span class="integer">10</span>: f_class = <span class="integer">3</span>
            <span class="keyword">elif</span> <span class="integer">11</span> &lt;= tok_freq &lt;= <span class="integer">20</span>: f_class = <span class="integer">4</span>
            <span class="keyword">elif</span> <span class="integer">21</span> &lt;= tok_freq: f_class = <span class="integer">5</span>
<span class="error"> </span>
            block_occur = token_in_textblock(text, current_tok)
<span class="error"> </span>
            occurrences = df[df[<span class="string"><span class="delimiter">'</span><span class="content">Token</span><span class="delimiter">'</span></span>] == current_tok]                <span class="comment"># new dataframe containing all of curr_token's occurrences</span>
            previous_distance = distance_to_previous(tok_id, sent_id, occurrences)
<span class="error"> </span>
            features.iat[tok_id, <span class="integer">5</span>] = current_pos + <span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span> + <span class="predefined">str</span>(f_class) + <span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span> + <span class="predefined">str</span>(block_occur) + <span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span> + <span class="predefined">str</span>(previous_distance)
<span class="error"> </span>
            tok_count += <span class="integer">1</span>
<span class="error"> </span>
    <span class="keyword">return</span> features</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output is a DataFrame that looks like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>           CurrToken       PrevToken       NextToken     TokenTags LengthPosition TagFreqOccur
0                Den             NaN     Mittelgrund    ART-NaN-NN            6-1    ART-2-1-1
1        Mittelgrund             Den          bilden      NN-ART-V            6-1     NN-1-1-1
2             bilden     Mittelgrund          Säulen       V-NN-NN            6-1      V-1-1-1
3             Säulen          bilden             mit       NN-V-PP            6-3     NN-1-1-1
4                mit          Säulen          weiten     PP-NN-ADJ            6-3     PP-3-3-1
5             weiten             mit  Zwischenräumen     ADJ-PP-NN            6-3    ADJ-1-1-1
6     Zwischenräumen          weiten               ,   NN-ADJ-PUNC            6-3     NN-1-1-1
7                  ,  Zwischenräumen             das   PUNC-NN-ART            6-3   PUNC-5-4-1
8                das               ,        Peristyl   ART-PUNC-NN            6-3    ART-3-3-1
9           Peristyl             das     bezeichnend    NN-ART-ADJ            6-2     NN-1-1-1
10       bezeichnend        Peristyl               .   ADJ-NN-PUNC            6-2    ADJ-1-1-1
11                 .     bezeichnend              Im   PUNC-ADJ-PP            6-2   PUNC-5-4-1
12                Im               .    Hintergrunde    PP-PUNC-NN            6-1     PP-2-2-1
13      Hintergrunde              Im             der     NN-PP-ART            6-1     NN-1-1-1
14               der    Hintergrunde          Tempel     ART-NN-NN            6-1    ART-5-4-1
15            Tempel             der               ,   NN-ART-PUNC            6-3     NN-3-3-1
16                 ,          Tempel              zu    PUNC-NN-PP            6-3   PUNC-5-4-3
17                zu               ,             dem    PP-PUNC-PR            6-3     PP-4-4-1
18               dem              zu         mehrere      PR-PP-PR            6-3     PR-3-3-1
19           mehrere             dem          Stufen      PR-PR-NN            6-3     PR-2-2-1
20            Stufen         mehrere     emporführen       NN-PR-V            6-2     NN-2-1-1
21       emporführen          Stufen               .     V-NN-PUNC            6-2      V-1-1-1
22                 .     emporführen            Nach     PUNC-V-PP            6-2   PUNC-5-4-3
23              Nach               .           vorne   PP-PUNC-ADV            6-1     PP-2-2-1
24             vorne            Nach               ,   ADV-PP-PUNC            6-1    ADV-1-1-1
25                 ,           vorne          rechts  PUNC-ADV-ADV            6-1   PUNC-5-4-3
26            rechts               ,             die  ADV-PUNC-ART            6-3    ADV-1-1-1
27               die          rechts          Statue    ART-ADV-NN            6-3    ART-5-4-1
28            Statue             die           Amors     NN-ART-NP            6-3     NN-1-1-1
29             Amors          Statue               ,    NP-NN-PUNC            6-3     NP-1-1-1
...              ...             ...             ...           ...            ...          ...</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_preparing_the_data_2">9.4. Preparing the Data</h3>
<div class="paragraph">
<p>What we need to do now, is to gather this information in bulk and
convert it into a form suitable for training, respectively testing a
classifier. In order to achieve this, we write a function that loops
over all CSV files in a directory and feeds them into <em>featureselect()</em>
one by one. For each document, the resulting feature table gets trimmed
down to <em>n</em> randomly selected observations (rows) and appended to a big
DataFrame, which will become the input matrix <em>X</em>  for the
classification task. Simultaneously we build up a vector <em>y</em>, holding
the corresponding author label for each observation. Next, the big
DataFrame needs to be vectorized, e.g. converted from strings into
numbers by use of a dictionary. This takes every distinct entry in the
table and turns it into a column filled with 0&#8217;s and occasional 1&#8217;s for
each time the encoded value shows up in a row. As one can imagine, the
outcome is a table where the data is scattered among a lot of zeros,
also called a <em>sparse matrix</em>. For the classifier<em> </em>to accept the
data, we also need to make sure the matrix doesn&#8217;t contain missing
values and use an imputer function that replaces NaN&#8217;s by the median of
their respective rows.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">preprocessing</span>(path, n):
    feats = []
    y = []
<span class="error"> </span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">processing files and randomly selecting {0} features each ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.format(n))
<span class="error"> </span>
    <span class="keyword">for</span> <span class="predefined">file</span> <span class="keyword">in</span> os.listdir(path=path):
        <span class="keyword">if</span> <span class="keyword">not</span> <span class="predefined">file</span>.startswith(<span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>):
            author = <span class="predefined">file</span>.split(<span class="string"><span class="delimiter">&quot;</span><span class="content">-</span><span class="delimiter">&quot;</span></span>)[<span class="integer">0</span>].replace(<span class="string"><span class="delimiter">&quot;</span><span class="content">%20</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>)
            filepath = path+<span class="string"><span class="delimiter">&quot;</span><span class="content">/</span><span class="delimiter">&quot;</span></span>+<span class="predefined">file</span>
            print(filepath)
<span class="error"> </span>
            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(n): y.append(author)                     <span class="comment"># add n labels to y</span>
<span class="error"> </span>
            <span class="keyword">with</span> <span class="predefined">open</span>(filepath, <span class="string"><span class="delimiter">&quot;</span><span class="content">r</span><span class="delimiter">&quot;</span></span>) <span class="keyword">as</span> f:
                feat = featureselect(f)                             <span class="comment"># perform feature selection</span>
                rows = np.random.choice(feat.index.values, n)       <span class="comment"># randomly select n observations</span>
                feat_rand = feat.ix[rows]
<span class="error"> </span>
                feats.append(feat_rand)
                f.close()
<span class="error"> </span>
    data = pd.concat(feats, ignore_index=<span class="predefined-constant">True</span>)                      <span class="comment"># merge into one dataframe</span>
<span class="error"> </span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">dimensions of X: {0}</span><span class="delimiter">&quot;</span></span>.format(data.shape))
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">dimensions of y: {0}</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.format(<span class="predefined">len</span>(y)))
<span class="error"> </span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">vectorizing ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
<span class="error"> </span>
    vec = DictVectorizer(sparse=<span class="predefined-constant">False</span>)
    X = vec.fit_transform(data.T.to_dict().values())
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">dimensions of X after vectorization: {0}</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.format(X.shape))
<span class="error"> </span>
    imp = Imputer(missing_values=<span class="string"><span class="delimiter">'</span><span class="content">NaN</span><span class="delimiter">'</span></span>, strategy=<span class="string"><span class="delimiter">'</span><span class="content">median</span><span class="delimiter">'</span></span>, axis=<span class="integer">0</span>)    <span class="comment"># replace NaN</span>
    X = imp.fit_transform(X)
<span class="error"> </span>
    <span class="keyword">return</span> X, y, vec</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_training_and_evaluating_the_classifier">9.5. Training and Evaluating the Classifier</h3>
<div class="paragraph">
<p>Now, we can put it all together - first we check for two arguments,
a folder containing CSV files for training and one file for testing the
classifier. The folder gets passed on to the preprocessing() function,
which returns the input matrix <em>X</em>, the label vector <em>y</em>, plus - as
prerequisite for the prediction step later on - the dictionary used to
vectorize <em>X</em>. Next,
the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">RandomForestClassifier</a>
can be trained by providing the data and a number of parameters, here we
use the number of trees in the model and the number of allowed
concurrent processing threads. As specified in van Halteren et al., each
model should be _"<em>_trained on a collection of 11200 (2 authors x 8
training texts x 700 observations) feature vectors".</em> The 8 training
texts are part of a set of 9 texts for each author and comprise 3
different genres (see the
<a href="#ExampleCorpus.2">corpus
description</a>).<em> </em>The number of observations can be traced back to
properties of the originally used algorithm, but it is also a sensible
default value for this adaption of the experiment.</p>
</div>
<div class="paragraph">
<p>Following training, an evaluation of the model using the
scikit-learn <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation">cross
validation function</a> is performed. It is set up to use five randomly
shuffled train and test sets in order to calculate a mean accuracy for
the classifier.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">n_obs = <span class="integer">700</span>                                                         <span class="comment"># no. of observations to select</span>
n_trees = <span class="integer">30</span>                                                        <span class="comment"># no. of estimators in RandomForestClassifier</span>
<span class="error"> </span>
<span class="keyword">if</span> <span class="predefined">len</span>(sys.argv) &lt; <span class="integer">3</span>:
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">usage: {0} [folder containing csv files for training] [csv file for testing]</span><span class="delimiter">&quot;</span></span>.format(sys.argv[<span class="integer">0</span>]))
    sys.exit(<span class="integer">1</span>)
<span class="error"> </span>
<span class="comment"># do feature selection, normalization, and vectorization</span>
X, y, vec = preprocessing(sys.argv[<span class="integer">1</span>], n_obs)
<span class="error"> </span>
<span class="comment"># model training</span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">training classifier ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
clf = RandomForestClassifier(n_estimators=n_trees, n_jobs=-<span class="integer">1</span>).fit(X, y) <span class="comment"># -1 sets n_jobs to the number of CPU cores</span>
print(clf)
<span class="error"> </span>
<span class="comment"># evaluation</span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">performing cross validation (n_iter=5, test_size=0.125) ...</span><span class="delimiter">&quot;</span></span>)
cv = ShuffleSplit(X.shape[<span class="integer">0</span>], n_iter=<span class="integer">5</span>, test_size=<span class="float">0.125</span>, random_state=<span class="integer">4</span>)
scores = cross_val_score(clf, X, y, cv=cv, n_jobs=-<span class="integer">1</span>)
print(scores)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">mean accuracy: %0.2f (+/- %0.2f)</span><span class="char">\n</span><span class="delimiter">&quot;</span></span><span class="error"> </span>% (scores.mean(), scores.std() * <span class="integer">2</span>))</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Output:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre>processing files and randomly selecting 700 features each ...

train/Grillparzer%20-%20Das%20goldene%20Vließ%20(Anfang)%20(Drama).txt.csv
train/Grillparzer%20-%20Das%20Kloster%20bei%20Sendomir%20(Anfang)%20(Prosa).txt.csv
train/Grillparzer%20-%20Der%20arme%20Spielmann%20(Anfang)%20(Prosa).txt.csv
train/Grillparzer%20-%20Der%20Traum%20ein%20Leben%20(Anfang)%20(Drama).txt.csv
train/Grillparzer%20-%20Ein%20Erlebnis%20(Prosa).txt.csv
train/Grillparzer%20-%20Gedichte%201%20(Lyrik).txt.csv
train/Grillparzer%20-%20Gedichte%202%20(Lyrik).txt.csv
train/Grillparzer%20-%20Gedichte%203%20(Lyrik).txt.csv
train/von%20Kleist%20-%20Amphitryon%20(Anfang)%20(Drama).txt.csv
train/von%20Kleist%20-%20An%20Wilhelmine%20(Lyrik).txt.csv
train/von%20Kleist%20-%20Das%20Bettelweib%20von%20Locarno%20(Prosa).txt.csv
train/von%20Kleist%20-%20Das%20Erdbeben%20in%20Chili%20(Prosa).txt.csv
train/von%20Kleist%20-%20Das%20Käthchen%20von%20Heilbronn%20(Anfang)%20(Drama).txt.csv
train/von%20Kleist%20-%20Der%20Welt%20Lauf%20(Lyrik).txt.csv
train/von%20Kleist%20-%20Der%20zerbrochne%20Krug%20(Anfang)%20(Drama).txt.csv
train/von%20Kleist%20-%20Die%20beiden%20Tauben%20(Lyrik).txt.csv

dimensions of X: (11200, 6)
dimensions of y: 11200

vectorizing ...

dimensions of X after vectorization: (11200, 9692)

training classifier ...

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)

performing cross validation (n_iter=5, test_size=0.125) ...
[ 0.76214286  0.75928571  0.76714286  0.75142857  0.75785714]
mean accuracy: 0.76 (+/- 0.01)</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_prediction_using_the_classifier">9.6. Prediction Using the Classifier</h3>
<div class="paragraph">
<p>Finally, we can use the trained classifier object to predict which
author the text can be attributed to. The test text - which should be
the 9th text from one author&#8217;s set and was not included in training the
model - is sent through the same pre-processing steps as the other texts
before. What matters here, is that we use the original classifier
and <em>DictVectorizer</em> objects to vectorize and classify the test text.</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> You can also decouple the prediction from the training part by
using an already trained classifier object.
See <a href="http://scikit-learn.org/stable/modules/model_persistence.html">model
persistence</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">print(<span class="string"><span class="delimiter">&quot;</span><span class="content">predicting author for {0} ...</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.format(sys.argv[<span class="integer">2</span>]))
<span class="error"> </span>
<span class="comment"># feature selection and preprocessing for testfile</span>
<span class="keyword">with</span> <span class="predefined">open</span>(sys.argv[<span class="integer">2</span>], <span class="string"><span class="delimiter">&quot;</span><span class="content">r</span><span class="delimiter">&quot;</span></span>) <span class="keyword">as</span> f:
    feat = featureselect(f)                             <span class="comment"># perform feature selection</span>
    rows = np.random.choice(feat.index.values, n_obs)       <span class="comment"># randomly select n observations</span>
    feat = feat.ix[rows]
<span class="error"> </span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">dimensions of X_test: {0}</span><span class="delimiter">&quot;</span></span>.format(feat.shape))
<span class="error"> </span>
X_test = vec.transform(feat.T.to_dict().values())       <span class="comment"># vec must be the same DictVectorizer object as generated by preprocessing()</span>
<span class="error"> </span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">dimensions of X_test after vectorization: {0}</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.format(X_test.shape))
<span class="error"> </span>
imp = Imputer(missing_values=<span class="string"><span class="delimiter">'</span><span class="content">NaN</span><span class="delimiter">'</span></span>, strategy=<span class="string"><span class="delimiter">'</span><span class="content">median</span><span class="delimiter">'</span></span>, axis=<span class="integer">0</span>)    <span class="comment"># replace NaN</span>
X_test = imp.fit_transform(X_test)
<span class="error"> </span>
<span class="comment"># prediction</span>
y_pred = clf.predict(X_test)
<span class="error"> </span>
c = Counter(y_pred)
c_key = <span class="predefined">list</span>(c.keys())
c_val = <span class="predefined">list</span>(c.values())
print(c_key[<span class="integer">0</span>], c_val[<span class="integer">0</span>]/(<span class="predefined">sum</span>(c.values())/<span class="integer">100</span>), <span class="string"><span class="delimiter">&quot;</span><span class="content">% - </span><span class="delimiter">&quot;</span></span>,
      c_key[<span class="integer">1</span>], c_val[<span class="integer">1</span>]/(<span class="predefined">sum</span>(c.values())/<span class="integer">100</span>), <span class="string"><span class="delimiter">&quot;</span><span class="content">%</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Output:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre>predicting author for test/von%20Kleist%20-%20Der%20Findling%20(Prosa).txt.csv ...

dimensions of X_test: (700, 6)
dimensions of X_test after vectorization: (672, 9692)

von Kleist  77.52976190476191 % -  Grillparzer  22.470238095238095 %</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Note:</strong> During vectorization, Python raises a warning because
observations which cannot be found in the dictionary, have to be
dropped. This is in fact how it should behave and if you want to
suppress those warnings, you can append the following to the import
statements:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">warnings</span>
warnings.filterwarnings(<span class="string"><span class="delimiter">&quot;</span><span class="content">ignore</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Discussion:</strong></p>
</div>
<div class="paragraph">
<p>To wrap up, in this recipe we built a genre-independent
2-author-classifier using only the feature set from van Halteren et
al.'s paper. While we did use neither the original algorithm, nor had a
similarly controlled corpus at our disposal, the classifier displays an
accuracy of around 0.70. Further tests will be needed to assess its
cross-genre properties and accuracy in different settings. Furthermore,
to really recreate the paper&#8217;s experimental setup, one would need to
train classifiers for all possible pairs in a set of 8 authors and
derive a combined classification score from that. All in all it is an
encouraging start, though - the features as specified in the paper seem
to be rather robust to different text types and might in fact show, that
an individually measurable human stylome in writing exists. Apart from
this experimental setting and prototypical authorship attribution
problem, another possible application for such a high granularity
classifier in the context of literary studies could be to measure
stylistic differences within and in-between one author&#8217;s works (e.g. in
order to reveal differences in narrators or focalizations).</p>
</div>
<div class="paragraph">
<p>We really encourage trying out different classifiers and parameters for
this task. We have tried most which are included with scikit-learn and
found that apart from Random Forests, the
<a href="http://scikit-learn.org/stable/modules/tree.html">Decision Tree</a> and the
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">Gaussian
Naive Bayes</a> classifier perform pretty well. Let us know if you find
other models and/or interesting parameter settings to work with and we
will list them here.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_example_recipe_network_visualization_in_python">10. Example Recipe: Network Visualization in Python</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following example attempts to show how to create a simple social network visualization of German poets by using text files extracted from Wikipedia. The <a href="https://pypi.python.org/pypi/wikipedia">Wikipedia API</a> for Python is used to scrape the content from Wikipedia as <strong>plain text</strong>. With the help of the <a href="https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/releases">DARIAH-DKPro-Wrapper</a> we gain access to the <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entities</a> (NE) in each file, compare them using basic Python programming and finally visualize them with the Python <a href="https://networkx.github.io">NetworkX</a> and <a href="http://matplotlib.org">matplotlib</a> packages.
The basic assumption is that a connection between two authors exists if there is a certain amount of overlap in the Named Entities we extracted from their Wikipedia articles.
Every author is represented by a node, a connection between two authors by an edge which is created when the number of overlaps passes a certain threshold.</p>
</div>
<div class="paragraph">
<p>You can find the ready-to-run scripts for this recipe <a href="https://github.com/severinsimmler/DARIAH-Network-Visualization">here</a>.</p>
</div>
<div class="sect2">
<h3 id="_setting_up_the_environment_3">10.1. Setting Up the Environment</h3>
<div class="paragraph">
<p>As explained in the <a href="#SettinguptheEnvironment">example above</a> you have to install three packages to realize this recipe.
Issue the following command in the command line to download and install the needed packages:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>pip3 install wikipedia
pip3 install networkx</pre>
</div>
</div>
<div class="paragraph">
<p>Also make sure the package <em>matplotlib</em> is installed.</p>
</div>
</div>
<div class="sect2">
<h3 id="_crawling_wikipedia">10.2. Crawling Wikipedia</h3>
<div class="paragraph">
<p>The first part of the recipe is designed for interactive use. It is recommended to copy the following code into a text file and interpret it with Python through the command prompt. For more clearness the whole script is divided into small parts with explanations on what is going on in the single parts.
Use the following <code>import</code> statements in your first script after the first line:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">wikipedia</span>
<span class="keyword">import</span> <span class="include">re</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>In the following part we will create a new text file including a list of authors:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">create_authors</span>(working_directory, wiki_page, wiki_section):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Gathers names from Wikipedia</span><span class="delimiter">&quot;&quot;&quot;</span></span>

    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Creating authors.txt ...</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">with</span> <span class="predefined">open</span>(working_directory + <span class="string"><span class="delimiter">&quot;</span><span class="content">/authors.txt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">w</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">'</span><span class="content">utf-8</span><span class="delimiter">'</span></span>) <span class="keyword">as</span> authors:
        full_content = wikipedia.page(wiki_page)
        selected_content = full_content.section(wiki_section)
        only_name = re.sub(<span class="string"><span class="delimiter">&quot;</span><span class="content">[ </span><span class="char">\t</span><span class="char">\r</span><span class="char">\n</span><span class="char">\f</span><span class="content">]+[</span><span class="content">\(</span><span class="content">\[</span><span class="content">].*?[</span><span class="content">\]</span><span class="content">\)</span><span class="content">]</span><span class="delimiter">&quot;</span></span>,<span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>, selected_content)  <span class="comment"># erases characters after full name</span>
        authors.write(only_name)
        print(only_name)</code></pre>
</div>
</div>
<div class="paragraph">
<p>As Wikipedia happens to consist of living documents, we provide a snapshot of a list of authors <a href="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/author.txt">here</a>.</p>
</div>
<div class="paragraph">
<p>Alternatively, you can create your own list of authors (make sure you use the exact name used by Wikipedia).</p>
</div>
<div class="paragraph">
<p><strong>Output:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre>Creating authors.txt ...
Dietmar von Aist
Friedrich von Hausen
Heinrich von Rugge
Heinrich von Veldeke
Herger
Der von Kürenberg
Meinloh von Sevelingen
Rudolf von Fenis
Spervogel</pre>
</div>
</div>
<div class="paragraph">
<p>To crawl the Wikipedia database with your determined authors list, add the following code to your script:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">crawl_wikipedia</span>(authors_file, output_directory):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Crawls Wikipedia with authors.txt</span><span class="delimiter">&quot;&quot;&quot;</span></span>

    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Crawling Wikipedia ...</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">with</span> <span class="predefined">open</span>(authors_file, <span class="string"><span class="delimiter">&quot;</span><span class="content">r</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>) <span class="keyword">as</span> authors:
        <span class="keyword">for</span> author <span class="keyword">in</span> authors.read().splitlines():
            <span class="keyword">try</span>:
                page_title = wikipedia.page(author)
                <span class="keyword">if</span> page_title:
                    <span class="keyword">with</span> <span class="predefined">open</span>(output_directory + <span class="string"><span class="delimiter">&quot;</span><span class="content">/</span><span class="delimiter">&quot;</span></span> + author + <span class="string"><span class="delimiter">&quot;</span><span class="content">.txt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">w</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">'</span><span class="content">utf-8</span><span class="delimiter">'</span></span>) <span class="keyword">as</span> new_author:
                        new_author.write(page_title.content)
                        print(author + <span class="string"><span class="delimiter">&quot;</span><span class="content">: saved</span><span class="delimiter">&quot;</span></span>)

                <span class="keyword">else</span>:
                    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Error: Cannot create variable for wikipedia.page</span><span class="delimiter">&quot;</span></span>)

            <span class="keyword">except</span> wikipedia.exceptions.DisambiguationError:
                <span class="keyword">pass</span>
            <span class="keyword">except</span> wikipedia.exceptions.HTTPTimeoutError:
                <span class="keyword">pass</span>
            <span class="keyword">except</span> wikipedia.exceptions.RedirectError:
                <span class="keyword">pass</span>
            <span class="keyword">except</span> wikipedia.exceptions.PageError:
                <span class="keyword">pass</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Output:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre>Crawling Wikipedia ...
Dietmar von Aist: saved
Friedrich von Hausen: saved
Heinrich von Rugge: saved
Heinrich von Veldeke: saved
Herger: saved
Der von Kürenberg: saved
Meinloh von Sevelingen: saved
Rudolf von Fenis: saved
Spervogel: saved</pre>
</div>
</div>
<div class="paragraph">
<p>Finally we are putting everything together. In case you have worked with the <code>create_authors</code> function use the following <code>main()</code> part:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">main</span>(working_directory, output_directory, wiki_page, wiki_section):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    :param working_directory: e.g. /users/networks</span><span class="content">
</span><span class="content">    :param output_directory: e.g. /users/networks/wikis</span><span class="content">
</span><span class="content">    :param wiki_page: e.g. &quot;Liste deutschsprachiger Lyriker&quot;</span><span class="content">
</span><span class="content">    :param wiki_section: e.g. &quot;12. Jahrhundert&quot;</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>

    wikipedia.set_lang(<span class="string"><span class="delimiter">&quot;</span><span class="content">de</span><span class="delimiter">&quot;</span></span>)    <span class="comment"># change language</span>
    create_authors(working_directory, wiki_page, wiki_section)
    crawl_wikipedia(sys.argv[<span class="integer">1</span>] + <span class="string"><span class="delimiter">&quot;</span><span class="content">/authors.txt</span><span class="delimiter">&quot;</span></span>, output_directory)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    <span class="keyword">import</span> <span class="include">sys</span>
    main(sys.argv[<span class="integer">1</span>], sys.argv[<span class="integer">2</span>], sys.argv[<span class="integer">3</span>], sys.argv[<span class="integer">4</span>])</code></pre>
</div>
</div>
<div class="paragraph">
<p>To run the script type the following command in your command line:</p>
</div>
<div class="paragraph">
<p><code>python3 script workingdirectory outputdirectory "wikipage" "wikisection"</code></p>
</div>
<div class="paragraph">
<p>and press Enter.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p><code>python3 /users/networks/crawler.py /users/networks /users/networks/wikis "Liste deutschsprachiger Lyriker" "12. Jahrhundert"</code></p>
</div>
<div class="paragraph">
<p>In case you already had a text file like <em>authors.txt</em> use the following <code>main()</code> part:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">main</span>(authors_file, output_directory):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    :param authors_file: e.g. /users/networks/my_own_file.txt</span><span class="content">
</span><span class="content">    :param output_directory: e.g. /users/networks/wikis</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>

    wikipedia.set_lang(<span class="string"><span class="delimiter">&quot;</span><span class="content">de</span><span class="delimiter">&quot;</span></span>)    <span class="comment"># change language</span>
    crawl_wikipedia(authors_file, output_directory)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    <span class="keyword">import</span> <span class="include">sys</span>
    main(sys.argv[<span class="integer">1</span>], sys.argv[<span class="integer">2</span>])</code></pre>
</div>
</div>
<div class="paragraph">
<p>To run the script type the following command in your command line:</p>
</div>
<div class="paragraph">
<p><code>python3 script authorsfile outputdirectory</code></p>
</div>
<div class="paragraph">
<p>and press Enter.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p><code>python3 /users/networks/crawler.py /users/networks/my_own_file.txt /users/networks/wikis</code></p>
</div>
<div class="paragraph">
<p>If everything worked fine you should have one text file <strong>authors.txt</strong> containing a list of names in your working directory. In your output folder there should be one text file for each author listed in <strong>authors.txt</strong> containing the specific Wikipedia page.</p>
</div>
</div>
<div class="sect2">
<h3 id="_using_dkpro_wrapper_and_networkx_to_visualize_networks">10.3. Using DKPro Wrapper and NetworkX to Visualize Networks</h3>
<div class="paragraph">
<p>In the second part of the recipe you will analyze your previously created text files with the DKPro-Wrapper.
How to process a collection of files in the same folder is explained <a href="#InputFolders">further above</a>.
After creating a <strong>.csv file</strong> for each text file you use Python for further work on your files. Make sure you import the different modules first.
Create the second (and last) script starting after the first line with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">csv</span>
<span class="keyword">from</span> <span class="include">collections</span> <span class="keyword">import</span> <span class="include">defaultdict</span>
<span class="keyword">import</span> <span class="include">itertools</span>
<span class="keyword">import</span> <span class="include">glob</span>
<span class="keyword">import</span> <span class="include">os</span>
<span class="keyword">import</span> <span class="include">networkx</span> <span class="keyword">as</span> nx
<span class="keyword">import</span> <span class="include">matplotlib.pyplot</span> <span class="keyword">as</span> plt
<span class="keyword">import</span> <span class="include">re</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The following function ingests the annotated file and extracts every NE. In the process first name and last name(s) or base name and extensions are merged. The <strong>.csv file</strong> marks first names and base names as B-PER and last names and extensions as I-PER. The function saves both B-PER and I-PER in a dictionary. Only B-PER or a B-PER followed by any combination of I-PER will be saved as one full name.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">ne_count</span>(input_file):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Extracts only Named Entities</span><span class="delimiter">&quot;&quot;&quot;</span></span>

    ne_counter = defaultdict(<span class="predefined">int</span>)
    <span class="keyword">with</span> <span class="predefined">open</span>(input_file, encoding=<span class="string"><span class="delimiter">'</span><span class="content">utf-8</span><span class="delimiter">'</span></span>) <span class="keyword">as</span> csv_file:
        read_csv = csv.DictReader(csv_file, delimiter=<span class="string"><span class="delimiter">'</span><span class="char">\t</span><span class="delimiter">'</span></span>, quoting=csv.QUOTE_NONE)
        lemma = []

        <span class="keyword">for</span> row <span class="keyword">in</span> read_csv:
            <span class="keyword">if</span> row[<span class="string"><span class="delimiter">'</span><span class="content">NamedEntity</span><span class="delimiter">'</span></span>] != <span class="string"><span class="delimiter">&quot;</span><span class="content">_</span><span class="delimiter">&quot;</span></span> <span class="keyword">and</span> row[<span class="string"><span class="delimiter">'</span><span class="content">CPOS</span><span class="delimiter">'</span></span>] != <span class="string"><span class="delimiter">&quot;</span><span class="content">PUNC</span><span class="delimiter">&quot;</span></span>:
                lemma.append(row[<span class="string"><span class="delimiter">'</span><span class="content">Lemma</span><span class="delimiter">'</span></span>])
            <span class="keyword">else</span>:
                <span class="keyword">if</span> lemma:
                    joined_lemma = <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>.join(lemma)
                    ne_counter[joined_lemma] += <span class="integer">1</span>
                    lemma = []
    <span class="keyword">return</span> ne_counter</code></pre>
</div>
</div>
<div class="paragraph">
<p>This one is used to compare the dictionaries created above. It returns the number of matches which will be used to determine if an edge between two authors will be drawn:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">compare_ne_counter</span>(ne_dict1, ne_dict2):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Compares two dictionaries</span><span class="delimiter">&quot;&quot;&quot;</span></span>

    weight = <span class="integer">0</span>
    <span class="keyword">for</span> key <span class="keyword">in</span> ne_dict1.keys():
        <span class="keyword">if</span> key <span class="keyword">in</span> ne_dict2.keys():
            weight += <span class="integer">1</span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">this is the weight: </span><span class="delimiter">&quot;</span></span> + <span class="predefined">str</span>(weight))
    <span class="keyword">return</span> weight</code></pre>
</div>
</div>
<div class="paragraph">
<p>To label the nodes for the graph, this function extracts the names by removing the extensions of each author&#8217;s file name:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">extract_basename</span>(file_path):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Extracts names from file names</span><span class="delimiter">&quot;&quot;&quot;</span></span>

    file_name_txt_csv = os.path.basename(file_path)
    file_name_txt = os.path.splitext(file_name_txt_csv)
    file_name = os.path.splitext(file_name_txt[<span class="integer">0</span>])
    <span class="keyword">return</span> file_name[<span class="integer">0</span>]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, creating the graph:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">create_graph</span>(input_folder):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Creates graph including nodes and edges</span><span class="delimiter">&quot;&quot;&quot;</span></span>

    G = nx.Graph()
    file_list = glob.glob(input_folder)

    <span class="keyword">for</span> item <span class="keyword">in</span> file_list:
        G.add_node(extract_basename(item))

    <span class="keyword">for</span> a, b <span class="keyword">in</span> itertools.combinations(file_list, <span class="integer">2</span>):
        weight = compare_ne_counter(ne_count(a), ne_count(b))
        <span class="keyword">if</span> weight &gt; <span class="integer">10</span>:
            G.add_edge(extract_basename(a), extract_basename(b), {<span class="string"><span class="delimiter">'</span><span class="content">weight</span><span class="delimiter">'</span></span>: weight})
            <span class="comment"># create edges a-&gt;b (weight)</span>

    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Number of nodes:</span><span class="delimiter">&quot;</span></span>, G.number_of_nodes(), <span class="string"><span class="delimiter">&quot;</span><span class="content">  Number of edges: </span><span class="delimiter">&quot;</span></span>, G.number_of_edges())
    <span class="keyword">return</span> G</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Output:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre>this is the weight: 20
this is the weight: 11
this is the weight: 15
this is the weight: 7
this is the weight: 5
this is the weight: 9
this is the weight: 12
this is the weight: 18
this is the weight: 16
this is the weight: 10
this is the weight: 10
this is the weight: 14
this is the weight: 7
this is the weight: 8
this is the weight: 11
this is the weight: 9
this is the weight: 9
this is the weight: 9
this is the weight: 9
this is the weight: 8
this is the weight: 8
Number of nodes: 7   Number of edges:  8</pre>
</div>
</div>
<div class="paragraph">
<p>The following code lastly is the <code>main()</code> function, which calls the previously defined functions after having the user select an input and output folder:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">main</span>(input_folder, output_folder):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    :param input_folder: e.g. /users/networks/csv</span><span class="content">
</span><span class="content">    :param output_folder: e.g. /users/networks</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>

    G = create_graph(input_folder + <span class="string"><span class="delimiter">&quot;</span><span class="content">/*</span><span class="delimiter">&quot;</span></span>)
    <span class="comment"># If you want to create a circular graph, add '#' in front of every line of the following block,</span>
    <span class="comment"># erase the '#' of the three lines after 'Circular drawing', and run the script (again)</span>
    pos = nx.spring_layout(G)
    nx.draw_networkx_labels(G, pos, font_size=<span class="string"><span class="delimiter">'</span><span class="content">8</span><span class="delimiter">'</span></span>, font_color=<span class="string"><span class="delimiter">'</span><span class="content">r</span><span class="delimiter">'</span></span>)
    nx.draw_networkx_edges(G, pos, alpha=<span class="float">0.1</span>)
    plt.axis(<span class="string"><span class="delimiter">'</span><span class="content">off</span><span class="delimiter">'</span></span>)
    plt.savefig(output_folder + <span class="string"><span class="delimiter">&quot;</span><span class="content">/graph.png</span><span class="delimiter">&quot;</span></span>)

    <span class="comment"># Circular drawing:</span>
    <span class="comment"># nx.draw_circular(G, with_labels=True, alpha=0.3, font_size='8')</span>
    <span class="comment"># plt.axis('off')</span>
    <span class="comment"># plt.savefig(output_folder + &quot;/circular.png&quot;)</span>


<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    <span class="keyword">import</span> <span class="include">sys</span>
    main(sys.argv[<span class="integer">1</span>], sys.argv[<span class="integer">2</span>])</code></pre>
</div>
</div>
<div class="paragraph">
<p>To run the script type the following command in your command line:</p>
</div>
<div class="paragraph">
<p><code>python3 script inputfolder outputfolder</code></p>
</div>
<div class="paragraph">
<p>and press Enter.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p><code>python3 /users/networks/graph.py /users/networks/csv /users/networks</code></p>
</div>
<div class="paragraph">
<p><strong>Output:</strong></p>
</div>
<div class="paragraph">
<p>Your output is a <strong>.png file</strong> and should look like one of these.</p>
</div>
<div class="paragraph">
<p>Poets of the 12th century:
<span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/12th_century.png" alt="image"></span></p>
</div>
<div class="paragraph">
<p>Poets of the 13th century:
<span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/13th_century.png" alt="image"></span></p>
</div>
<div class="paragraph">
<p>In case you decided to draw a circular graph:
<span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/circular_new.png" alt="image"></span></p>
</div>
<div class="paragraph">
<p>This recipe also works with other languages, e.g. English. You have to update the main part of the <code>create_authors</code> function and one possible output could look like this for <code>"List of English-language poets" "A"</code>:
<span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/american_a.png" alt="image"></span></p>
</div>
<div class="paragraph">
<p><strong>Discussion:</strong>
In this recipe we created a visualization of an author&#8217;s social network using the output of the DARIAH-DKPro-Wrapper and two simple Python scripts to gain knowledge about the authors' relations. This is a great starting point for further research as the existing relations can now be examined more closely. The graphs might even hint at certain connections that have not been made yet. This kind of analysis therefore provides a groundwork and direction for further investigations on the topic.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_about_this_tutorial">11. About this Tutorial</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Contact:
<a href="https://wiki.de.dariah.eu/display/publicde/Cluster+5%3A+Quantitative+Datenanalyse">DARIAH-DE, Cluster 5 - Big Data in the Humanities</a></p>
</div>
<div class="paragraph">
<p>Comments are welcome, as are reports of bugs and typos.</p>
</div>
<div class="paragraph">
<p>We would like to acknowledge the following individuals for their
contributions to the continuing development of the tutorial:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Steffen Pielström</p>
</li>
<li>
<p>Stefan Pernes</p>
</li>
<li>
<p>Nils Reimers</p>
</li>
<li>
<p>Sina Bock</p>
</li>
<li>
<p>Philip Dürholt</p>
</li>
<li>
<p>Keli Du</p>
</li>
<li>
<p>Michael Huber</p>
</li>
<li>
<p>Severin Simmler</p>
</li>
<li>
<p>Thora Hagen</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All materials are published under
a <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons
Attribution 4.0 International</a> license (CC-BY 4.0).</p>
</div>
<div class="paragraph">
<p>These tutorials have been developed with support from
the <a href="http://de.dariah.eu/">DARIAH-DE</a> initiative, the German branch
of <a href="http://dariah.eu/">DARIAH-EU</a>, the European Digital Research
Infrastructure for the Arts and Humanities consortium. Funding has been
provided by the German Federal Ministry for Research and Education
(BMBF) under the identifier 01UG1110J.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/dariah-de_logo.png" alt="DARIAH"></span>
<span class="image"><img src="https://raw.githubusercontent.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/content/bmbf_logo.png" alt="BMBF"></span></p>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2020-06-18 16:50:54 CEST
</div>
</div>
</body>
</html>